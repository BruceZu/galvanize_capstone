{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_tools.py\n",
    "import json\n",
    "import codecs\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams, skipgrams\n",
    "import string\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_jsonl_file(path):\n",
    "    '''turn a jsonl file (carriage returns per record) into an array of objects'''\n",
    "    arr = []\n",
    "    f = codecs.open(path, 'r', 'utf-8')\n",
    "    for line in f:\n",
    "        record = json.loads(line.rstrip('\\n|\\r'))\n",
    "        arr.append(record)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def read_json_file(path):\n",
    "    '''Turn a normal json file (no carriage returns per record) into an object'''\n",
    "    text = codecs.open(path, 'r', 'utf-8').read()\n",
    "    return json.loads(text)\n",
    "\n",
    "\n",
    "def write_jsonl_file(list_of_objects, path):\n",
    "    '''Dump a list of objects out as a jsonl file'''\n",
    "    f = codecs.open(path, 'w', 'utf-8')\n",
    "    for row in list_of_objects:\n",
    "        json_record = json.dumps(row, ensure_ascii = False)\n",
    "        f.write(json_record + '\\n')\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "def write_json_file(obj, path):\n",
    "    '''Dump an object and write it out as json to a file'''\n",
    "    f = codecs.open(path, 'a', 'utf-8')\n",
    "    json_record = json.dumps(obj, ensure_ascii = False)\n",
    "    f.write(json_record + '\\n')\n",
    "    f.close\n",
    "    \n",
    "    \n",
    "def collection_to_df(collection):\n",
    "    '''\n",
    "    Returns a dataframe from a mongo collection.\n",
    "    '''\n",
    "    data = pd.DataFrame\n",
    "    for i in range(collection.count()):\n",
    "        data = data.append(pd.DataFrame.from_dict(collection[i], orient='index').T, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_bill_data():\n",
    "    '''\n",
    "    Query data from mongo db bills.bill_details and return a pandas dataframe.\n",
    "    \n",
    "    The data relevant to this project is currently set up to be limited to the \n",
    "    110th Congress forward.\n",
    "    --------------------\n",
    "    Parameters: None.\n",
    "    --------------------    \n",
    "    Returns: pandas dataframe with relevant data and corresponding labels.\n",
    "                \n",
    "    '''\n",
    "    # connect to mongodb\n",
    "    client = MongoClient() # defaults to localhost\n",
    "    db = client.bills\n",
    "    bill_info = db.bill_info\n",
    "    \n",
    "    # get mongoo data and convert mongo query resuls to dataframe\n",
    "    # need to execute query (.find) everytime i refer to it?\n",
    "    records_with_text = bill_info.find({'body': {'$regex': '(.+)'}})\n",
    "    data = pd.DataFrame(list(records_with_text))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # DATA CLEANUP\n",
    "    # filter out simple resolutions, concurrent resolutions, and amendments (for prelim model)\n",
    "    data = data[(data['leg_type'] != 'RESOLUTION') & (data['leg_type'] != 'CONCURRENT RESOLUTION') & (data['leg_type'] != 'AMENDMENT')].copy()\n",
    "    \n",
    "    # create column for character counts of the bill text\n",
    "    bill_lengths = list(map(lambda x: len(x), data['body']))\n",
    "    data['bill_char_counts'] = bill_lengths\n",
    "    \n",
    "    # convert date column to type datetime\n",
    "    data['intro_date'] = data['intro_date'].apply(lambda x: datetime.datetime.strptime(x[:10], '%m/%d/%Y'))\n",
    "\n",
    "    # strip out month from intro date\n",
    "    data['intro_month'] = data['intro_date'].apply(lambda x: x.month)\n",
    "    \n",
    "    # get session from year (odd years are Session 1, even years are Session 2)\n",
    "    data['session'] = data['congress_id'].apply(lambda x: 2 if int(x[:3])%2 == 0 else 1)\n",
    "    \n",
    "    # filter out non-numeric num_of_cosponsors: S. Rept. 110-184, TXT, All Actions\n",
    "    data = data[(data['num_of_cosponsors'] != 'S. Rept. 110-184') &\n",
    "               (data['num_of_cosponsors'] != 'TXT') &\n",
    "               (data['num_of_cosponsors'] != 'All Actions')].copy()\n",
    "    \n",
    "    # convert num_of_cosponsors to numeric\n",
    "    data['num_of_cosponsors'] = data['num_of_cosponsors'].apply(pd.to_numeric)\n",
    "    \n",
    "#     # correction for mislabeled sponsor_state and sponsor_party\n",
    "#     state = copy.copy(data['sponsor_state'])\n",
    "#     party = copy.copy(data['sponsor_party'])\n",
    "#     data['sponsor_state'] = party\n",
    "#     data['sponsor_party'] = state\n",
    "    \n",
    "    # create column for getting char_counts into buckets\n",
    "    data['char_count_bucket'] = None\n",
    "\n",
    "    d_0 = data[data['bill_char_counts'] <= 1000].copy()\n",
    "    d_1000 = data[(data['bill_char_counts'] > 1000) & (data['bill_char_counts'] <= 2000)].copy()\n",
    "    d_2000 = data[(data['bill_char_counts'] > 2000) & (data['bill_char_counts'] <= 3000)].copy()\n",
    "    d_3000 = data[(data['bill_char_counts'] > 3000) & (data['bill_char_counts'] <= 4000)].copy()\n",
    "    d_4000 = data[(data['bill_char_counts'] > 4000) & (data['bill_char_counts'] <= 5000)].copy()\n",
    "    d_5000 = data[(data['bill_char_counts'] > 5000) & (data['bill_char_counts'] <= 6000)].copy()\n",
    "    d_6000 = data[(data['bill_char_counts'] > 6000) & (data['bill_char_counts'] <= 7000)].copy()\n",
    "    d_7000 = data[(data['bill_char_counts'] > 7000) & (data['bill_char_counts'] <= 8000)].copy()\n",
    "    d_8000 = data[(data['bill_char_counts'] > 8000) & (data['bill_char_counts'] <= 9000)].copy()\n",
    "    d_9000 = data[(data['bill_char_counts'] > 9000) & (data['bill_char_counts'] <= 10000)].copy()\n",
    "    d_10000 = data[data['bill_char_counts'] > 10000].copy()\n",
    "\n",
    "\n",
    "    d_0['char_count_bucket'] = 'less than 1000'\n",
    "    d_1000['char_count_bucket'] = '1001 - 2000'\n",
    "    d_2000['char_count_bucket'] = '2001 - 3000'\n",
    "    d_3000['char_count_bucket'] = '3001 - 4000'\n",
    "    d_4000['char_count_bucket'] = '4001 - 5000'\n",
    "    d_5000['char_count_bucket'] = '5001 - 6000'\n",
    "    d_6000['char_count_bucket'] = '6001 - 7000'\n",
    "    d_7000['char_count_bucket'] = '7001 - 8000'\n",
    "    d_8000['char_count_bucket'] = '8001 - 9000'\n",
    "    d_9000['char_count_bucket'] = '9001 - 10000'\n",
    "    d_10000['char_count_bucket'] = 'greater than 10000'\n",
    "\n",
    "    data = pd.concat([d_0, d_1000, d_2000, d_3000, d_4000, d_5000, \n",
    "                      d_6000, d_7000, d_8000, d_9000, d_10000])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # LABELING\n",
    "    # break up dataframe into those that became law and others (did not or still pending)\n",
    "    became_law = data[(data['bill_status'] == 'Became Law') | (data['bill_status'] == 'Became Private Law')].copy()\n",
    "    others = data[(data['bill_status'] != 'Became Law') & (data['bill_status'] != 'Became Private Law')].copy()\n",
    "\n",
    "    became_law.loc[:, 'labels'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # break up others into current congress and previous ones. Anything that hasn't been signed into law\n",
    "    # before current session is dead. Currently, all bills vetoed by the president come from previous congresses\n",
    "    current_cong = others[others['congress_id'] == '115'].copy()\n",
    "    prev_cong = others[others['congress_id'] != '115'].copy()\n",
    "\n",
    "    prev_cong.loc[:, 'labels'] = 0\n",
    "\n",
    "\n",
    "\n",
    "    # let's label To President and Resolving Differences with 1. Everything else is on the floor\n",
    "    to_pres = current_cong[(current_cong['bill_status'] == 'To President') | (current_cong['bill_status'] == 'Resolving Differences')].copy()\n",
    "    on_floor = current_cong[(current_cong['bill_status'] != 'To President') & (current_cong['bill_status'] != 'Resolving Differences')].copy()\n",
    "\n",
    "    to_pres.loc[:, 'labels'] = 1\n",
    "\n",
    "\n",
    "    # break up bills on the floor to failed (0) and not failed\n",
    "    failed = on_floor[on_floor['bill_status'].str.startswith('Failed')].copy()\n",
    "    not_failed = on_floor[~on_floor['bill_status'].str.startswith('Failed')].copy()\n",
    "\n",
    "    failed.loc[:, 'labels'] = 0\n",
    "\n",
    "\n",
    "\n",
    "    # bills that haven't failed yet have either been just introduced or on their way\n",
    "    # label introduced with 'in_progress'. These will not be a part of our model.\n",
    "    introduced = not_failed[not_failed['bill_status'] == 'Introduced'].copy()\n",
    "    beyond_intro = not_failed[not_failed['bill_status'] != 'Introduced'].copy()\n",
    "\n",
    "    introduced.loc[:, 'labels'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "    # there are bills that started in one chamber and have already passed the other. We'll label\n",
    "    # these with a 1\n",
    "    passed_opp_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('S')) | \n",
    "                              (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('H'))].copy()\n",
    "\n",
    "    passed_opp_chamber.loc[:, 'labels'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # bills that are still in the chamber they were introduced in are 'in_progress'\n",
    "    in_orig_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('H')) | \n",
    "                              (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('S'))].copy()\n",
    "\n",
    "    in_orig_chamber.loc[:, 'labels'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "    # bring all the information back together\n",
    "    data_l = pd.concat([became_law, prev_cong, to_pres, failed, introduced, passed_opp_chamber, in_orig_chamber])\n",
    "\n",
    "\n",
    "    # filter out those that are still in progress\n",
    "    df = data_l[data_l['labels'] != 'in_progress'].copy()\n",
    "\n",
    "\n",
    "    # filter for most recent congress_ids\n",
    "    small_df = df[(df['congress_id'] == '115') | \n",
    "              (df['congress_id'] == '114') | \n",
    "              (df['congress_id'] == '113')| \n",
    "              (df['congress_id'] == '112')| \n",
    "              (df['congress_id'] == '111')| \n",
    "              (df['congress_id'] == '110')].copy()\n",
    "\n",
    "    \n",
    "    print('------------------')\n",
    "    print('------------------')\n",
    "    print('Data includes bills, joints resolutions, and laws with text from the 110th Congress (2007) to present')\n",
    "    print('Make changes in my_tools.get_bill_data to modify the data set.')\n",
    "    print('------------------')\n",
    "\n",
    "    \n",
    "    return small_df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "\n",
    "def process_corpus(df, corpus_col_name):\n",
    "    '''\n",
    "    Processes the text in df[corpus_col_name] to return a corpus (list) and the series of \n",
    "    corresponding labels in df[label_col_name].\n",
    "    \n",
    "    The intent of this function is to feed the output into a stratified train-test split.\n",
    "    -------------------\n",
    "    Parameters: df - pandas dataframe\n",
    "                corpus_col_name - name of column in df that contains the text to be processed.\n",
    "    -------------------\n",
    "    Returns: X - a list of documents\n",
    "             y - a pandas series of corresponding labels as int\n",
    "    '''\n",
    "    # create a corpus\n",
    "    print('------------------')\n",
    "    print('Creating corpus...')\n",
    "    documents = list(df[corpus_col_name])\n",
    "\n",
    "    # remove numbers\n",
    "    documents = list(map(lambda x: ' '.join(re.split('[,_\\d]+', x)), documents))\n",
    "    \n",
    "    # clip the intro of each bill\n",
    "    documents = list(map(lambda x: x[(x.index('Office]') + 8):], documents))\n",
    "\n",
    "    # tokenize the corpus\n",
    "    print('------------------')\n",
    "    print('Tokenizing...')\n",
    "    corpus = [word_tokenize(content.lower()) for content in documents]\n",
    "\n",
    "    # strip out the stop words from each \n",
    "    print('------------------')\n",
    "    print('Stripping out stop words, punctuation, and numbers...')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['mr', 'ms', 'mrs', 'said', 'year', 'would', 'could', 'also', 'shall', '_______________________________________________________________________'])\n",
    "    # print(stop_words)\n",
    "    corpus = [[token for token in doc if token not in stop_words] for doc in corpus]\n",
    "    # corpus[0]\n",
    "\n",
    "    # strip out the punctuation\n",
    "    punc = set(string.punctuation)\n",
    "    # print(punc)\n",
    "    corpus = [[token for token in doc if token not in punc] for doc in corpus]\n",
    "    # corpus[0]\n",
    "\n",
    "    # strip out the punctuation\n",
    "    string.digits\n",
    "\n",
    "\n",
    "    # lemmatize (and maybe stem?)\n",
    "    print('------------------')\n",
    "    print('Lemmatizing...')\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    corpus = [[lemmer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "    # corpus[0]\n",
    "\n",
    "    # build a vocabulary\n",
    "    print('------------------')\n",
    "    print('Creating a vocabulary...')\n",
    "    vocab_set = set()\n",
    "    [[vocab_set.add(token) for token in tokens] for tokens in corpus]\n",
    "    vocab = list(vocab_set)\n",
    "    # vocab[100000:100020]\n",
    "\n",
    "    # # for later model...\n",
    "    # # examine n-grams...\n",
    "    # # bigrams (two words side-by-side)\n",
    "    # print('------------------')\n",
    "    # print('Creating lists of bigrams, trigrams, skipgrams, etc...')\n",
    "    # bigrams = [list(ngrams(sequence = doc, n = 2)) for doc in corpus]\n",
    "    # trigrams = [list(ngrams(sequence = doc, n = 3)) for doc in corpus]\n",
    "    # #... more?\n",
    "\n",
    "    # # skipgrams (n-grams that skip k words)\n",
    "    # skipgrams = [list(skipgrams(sequence = doc, n = 2, k = 1)) for doc in corpus]\n",
    "\n",
    "\n",
    "    # rejoin each doc in corpus so each doc is a single string\n",
    "    corpus = [' '.join(tokens) for tokens in corpus]\n",
    "\n",
    "    print('------------------')\n",
    "    print('NLP preprocessing complete ...')\n",
    "\n",
    "    X = corpus\n",
    "    y = df['labels'].astype('int')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def plot_roc(X, y, clf_class, plot_name, **kwargs):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    n_splits=5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], y_prob[test_index, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    mean_tpr /= n_splits\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('17_' + plot_name + '.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from my_tools import *\n",
    "data = get_bill_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_lengths = list(map(lambda x: len(x), data['body']))\n",
    "data['bill_char_counts'] = bill_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus\n",
    "print('------------------')\n",
    "print('Creating corpus...')\n",
    "documents = list(data['body'])\n",
    "\n",
    "# remove numbers\n",
    "documents = list(map(lambda x: ' '.join(re.split('[,_\\d]+', x)), documents))\n",
    "\n",
    "len(documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
