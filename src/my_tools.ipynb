{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_tools.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_tools.py\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams, skipgrams\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    '''\n",
    "    Query data from mongo db bills.bill_details and return a pandas dataframe.\n",
    "    \n",
    "    The data relevant to this project is currently set up to be limited to the \n",
    "    110th Congress forward.\n",
    "    --------------------\n",
    "    Parameters: None.\n",
    "    --------------------    \n",
    "    Returns: pandas dataframe with relevant data and corresponding labels.\n",
    "                \n",
    "    '''\n",
    "    # connect to mongodb\n",
    "    client = MongoClient() # defaults to localhost\n",
    "    db = client.bills\n",
    "    bill_details = db.bill_details\n",
    "    \n",
    "    # get mongoo data and convert mongo query resuls to dataframe\n",
    "    # need to execute query (.find) everytime i refer to it?\n",
    "    records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "    data = pd.DataFrame(list(records_with_text))\n",
    "\n",
    "    # filter out simple resolutions, concurrent resolutions, and amendments (for prelim model)\n",
    "    data = data[(data['leg_type'] != 'RESOLUTION') & (data['leg_type'] != 'CONCURRENT RESOLUTION') & (data['leg_type'] != 'AMENDMENT')]\n",
    "\n",
    "    print('------------------')\n",
    "    print('Creating labels in column \\'passed\\'...')\n",
    "    \n",
    "    # break up dataframe into those that became law and others (did not or still pending)\n",
    "    became_law = data[(data['bill_status'] == 'Became Law') | (data['bill_status'] == 'Became Private Law')]\n",
    "    others = data[(data['bill_status'] != 'Became Law') & (data['bill_status'] != 'Became Private Law')]\n",
    "\n",
    "    became_law.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # break up others into current congress and previous ones. Anything that hasn't been signed into law\n",
    "    # before current session is dead. Currently, all bills vetoed by the president come from previous congresses\n",
    "    current_cong = others[others['congress_id'] == '115th']\n",
    "    prev_cong = others[others['congress_id'] != '115th']\n",
    "\n",
    "    prev_cong.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "    # let's label To President and Resolving Differences with 1. Everything else is on the floor\n",
    "    to_pres = current_cong[(current_cong['bill_status'] == 'To President') | (current_cong['bill_status'] == 'Resolving Differences')]\n",
    "    on_floor = current_cong[(current_cong['bill_status'] != 'To President') & (current_cong['bill_status'] != 'Resolving Differences')]\n",
    "\n",
    "    to_pres.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # break up bills on the floor to failed (0) and not failed\n",
    "    failed = on_floor[on_floor['bill_status'].str.startswith('Failed')]\n",
    "    not_failed = on_floor[~on_floor['bill_status'].str.startswith('Failed')]\n",
    "\n",
    "    failed.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "    # bills that haven't failed yet have either been just introduced or on their way\n",
    "    # label introduced with 'in_progress'. These will not be a part of our model.\n",
    "    introduced = not_failed[not_failed['bill_status'] == 'Introduced']\n",
    "    beyond_intro = not_failed[not_failed['bill_status'] != 'Introduced']\n",
    "\n",
    "    introduced.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "    # there are bills that started in one chamber and have already passed the other. We'll label\n",
    "    # these with a 1\n",
    "    passed_opp_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('S')) | \n",
    "                              (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('H'))]\n",
    "\n",
    "    passed_opp_chamber.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "    # bills that are still in the chamber they were introduced in are 'in_progress'\n",
    "    in_orig_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('H')) | \n",
    "                              (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('S'))]    \n",
    "\n",
    "    in_orig_chamber.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "    # bring all the information back together\n",
    "    data_l = pd.concat([became_law, prev_cong, to_pres, failed, introduced, passed_opp_chamber, in_orig_chamber])\n",
    "\n",
    "    # filter out those that are still in progress\n",
    "    df = data_l[data_l['passed'] != 'in_progress']\n",
    "\n",
    "    # filter for most recent congress_ids\n",
    "    small_df = df[(df['congress_id'] == '115th') | \n",
    "              (df['congress_id'] == '114th') | \n",
    "              (df['congress_id'] == '113th')| \n",
    "              (df['congress_id'] == '112th')| \n",
    "              (df['congress_id'] == '111th')| \n",
    "              (df['congress_id'] == '110th')]\n",
    "    \n",
    "    print('------------------')\n",
    "    print('------------------')\n",
    "    print('Data is from the 110th Congress (2007) to present')\n",
    "    print('------------------')\n",
    "    \n",
    "    return small_df\n",
    "\n",
    "\n",
    "\n",
    "def process_corpus(df, corpus_col_name, label_col_name):\n",
    "    '''\n",
    "    Processes the text in df[corpus_col_name] to return a corpus (list) and the series of \n",
    "    corresponding labels in df[label_col_name].\n",
    "    \n",
    "    The intent of this function is to feed the output into a stratified train-test split.\n",
    "    -------------------\n",
    "    Parameters: df - pandas dataframe\n",
    "                col_name - name of column in df that contains the text to be processed.\n",
    "    -------------------\n",
    "    Returns: X - a list of documents\n",
    "             y - a pandas series of corresponding labels\n",
    "    '''\n",
    "    # create a corpus\n",
    "    print('------------------')\n",
    "    print('Creating corpus...')\n",
    "    documents = list(df[corpus_col_name])\n",
    "\n",
    "    # remove numbers\n",
    "    documents = list(map(lambda x: ' '.join(re.split('[,_\\d]+', x)), documents))\n",
    "\n",
    "    # tokenize the corpus\n",
    "    print('------------------')\n",
    "    print('Tokenizing...')\n",
    "    corpus = [word_tokenize(content.lower()) for content in documents]\n",
    "\n",
    "    # strip out the stop words from each \n",
    "    print('------------------')\n",
    "    print('Stripping out stop words, punctuation, and numbers...')\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['mr', 'ms', 'mrs', 'said', 'year', 'would', 'could', 'also', 'shall', '_______________________________________________________________________'])\n",
    "    # print(stop_words)\n",
    "    corpus = [[token for token in doc if token not in stop_words] for doc in corpus]\n",
    "    # corpus[0]\n",
    "\n",
    "    # strip out the punctuation\n",
    "    punc = set(string.punctuation)\n",
    "    # print(punc)\n",
    "    corpus = [[token for token in doc if token not in punc] for doc in corpus]\n",
    "    # corpus[0]\n",
    "\n",
    "    # strip out the punctuation\n",
    "    string.digits\n",
    "\n",
    "\n",
    "    # lemmatize (and maybe stem?)\n",
    "    print('------------------')\n",
    "    print('Lemmatizing...')\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    corpus = [[lemmer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "    # corpus[0]\n",
    "\n",
    "    # build a vocabulary\n",
    "    print('------------------')\n",
    "    print('Creating a vocabulary...')\n",
    "    vocab_set = set()\n",
    "    [[vocab_set.add(token) for token in tokens] for tokens in corpus]\n",
    "    vocab = list(vocab_set)\n",
    "    # vocab[100000:100020]\n",
    "\n",
    "    # # for later model...\n",
    "    # # examine n-grams...\n",
    "    # # bigrams (two words side-by-side)\n",
    "    # print('------------------')\n",
    "    # print('Creating lists of bigrams, trigrams, skipgrams, etc...')\n",
    "    # bigrams = [list(ngrams(sequence = doc, n = 2)) for doc in corpus]\n",
    "    # trigrams = [list(ngrams(sequence = doc, n = 3)) for doc in corpus]\n",
    "    # #... more?\n",
    "\n",
    "    # # skipgrams (n-grams that skip k words)\n",
    "    # skipgrams = [list(skipgrams(sequence = doc, n = 2, k = 1)) for doc in corpus]\n",
    "\n",
    "\n",
    "    # rejoin each doc in corpus so each doc is a single string\n",
    "    corpus = [' '.join(tokens) for tokens in corpus]\n",
    "\n",
    "    print('------------------')\n",
    "    print('NLP preprocessing complete ...')\n",
    "\n",
    "    print('------------------')\n",
    "    print('Creating train-test split and vectorizing ...')\n",
    "    X = corpus\n",
    "    y = df[labels_col_name].astype('int')\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
