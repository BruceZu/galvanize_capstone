{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mongo_extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mongo_extract.py\n",
    "from pymongo import MongoClient\n",
    "import bson.json_util\n",
    "import copy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "def write_json_file(obj, path):\n",
    "    '''Dump an object and write it out as json to a file'''\n",
    "    f = codecs.open(path, 'a', 'utf-8')\n",
    "    json_record = json.dumps(obj, ensure_ascii = False)\n",
    "    f.write(json_record + '\\n')\n",
    "    f.close\n",
    "    \n",
    "    \n",
    "def soup_to_mongo(soup, collection_name):\n",
    "    '''There are 250 items in each soup object'''\n",
    "    # table of bills are in ol class\n",
    "    div = soup.find('div', {'class':'search-column-main'})\n",
    "    table = div.find('ol')\n",
    "\n",
    "    # iterate though each li class expanded to get rows\n",
    "    rows = table.find_all('li', {'class':'expanded'})\n",
    "    print('\\tThere are {} rows to iterate through on this pass.'.format(len(rows)))\n",
    "    \n",
    "\n",
    "    # store each row as key-value pair in a dictionary\n",
    "    empty_row = {'leg_id': None, \n",
    "                'leg_type': None,\n",
    "                'leg_url': None,\n",
    "                'intro_date': None,\n",
    "                'congress_id': None,\n",
    "                'desc': None,\n",
    "                'sponsor': None, \n",
    "                'sponsor_party': None, \n",
    "                'sponsor_state': None,\n",
    "                'sponsor_district': None,  #senators don't have districts\n",
    "                'num_of_cosponsors': None,\n",
    "#                 'cosponsors_url': None,\n",
    "                'cosponsors': None,  #requires navigation to another url and extracting names from table\n",
    "                'committee': None, \n",
    "                'bill_status': None,\n",
    "                'body': None   #requires navigation to another url\n",
    "                }\n",
    "\n",
    "\n",
    "    # for eyeball examination/debugging\n",
    "    # columns = rows[9].find_all('a')\n",
    "    # columns\n",
    "\n",
    "#     all_rows = []\n",
    "    i = 0\n",
    "\n",
    "    # iterate through each of the 'rows' to fill in the 'columns'\n",
    "    for row in rows:\n",
    "        new_row = copy.copy(empty_row)\n",
    "\n",
    "        # parse items within 'a' tag\n",
    "        columns = row.find_all('a')\n",
    "        if columns[0].text.strip() != '':\n",
    "            new_row['leg_id'] = columns[0].text.strip().replace('.', ' ')\n",
    "        if columns[0]['href'].strip() != '':\n",
    "            new_row['leg_url'] = columns[0]['href'].strip()\n",
    "        if columns[2].text.strip() != '':\n",
    "            new_row['num_of_cosponsors'] = columns[2].text.strip()\n",
    "            if new_row['num_of_cosponsors'] != '0':\n",
    "                # call function to get cosponsors table from url\n",
    "                cosponsors_url = columns[2]['href']\n",
    "                new_row['cosponsors'] = get_cosponsors(cosponsors_url)\n",
    "\n",
    "        # party, state, and district (for house reps) need to be stripped out of sponsor info\n",
    "        if columns[1].text.strip() != '':\n",
    "            rep = columns[1].text.strip()\n",
    "    #         print(rep)\n",
    "            new_row['sponsor'] = rep.rsplit('[', 1)[0][:-1]\n",
    "            party_dist = rep.rsplit('[', 1)[1][: -1]\n",
    "            party_dist_split = party_dist.split('-')\n",
    "            new_row['sponsor_state'] = party_dist_split[0]\n",
    "            new_row['sponsor_party'] = party_dist_split[1]\n",
    "            if len(party_dist_split) == 3:\n",
    "                new_row['sponsor_district'] = party_dist_split[2]\n",
    "\n",
    "        # parse items within 'span' tag\n",
    "        columns = row.find_all('span')\n",
    "        if columns[0].text != '':\n",
    "            new_row['leg_type'] = columns[0].text.strip()\n",
    "        if columns[1].text.strip().split()[2] != '':\n",
    "            new_row['congress_id'] = columns[1].text.strip().split()[2]\n",
    "        if columns[2].text != '':\n",
    "            new_row['desc'] = columns[2].text\n",
    "        if columns[4].text.strip()[12:] != '':\n",
    "            new_row['committee'] = columns[4].text.strip()[12:]\n",
    "        # date was a little tricky\n",
    "        dt = columns[3].text.strip().split()\n",
    "        if '(Introduced' in dt:\n",
    "            new_row['intro_date'] = dt[dt.index('(Introduced') + 1][:-1]\n",
    "\n",
    "        # parse items within p tag\n",
    "        columns = row.find_all('p')\n",
    "        if columns[0].text.strip()[25:] != '':\n",
    "            new_row['bill_status'] = columns[0].text.strip()[25:]\n",
    "\n",
    "#         all_rows.append(new_row)\n",
    "        collection_name.insert_one(new_row)\n",
    "        \n",
    "        i += 1\n",
    "        if i%20 == 0:\n",
    "            print('\\t\\t{:.2f}% complete'.format(100 * i / len(rows)))\n",
    "        \n",
    "#     return all_rows\n",
    "\n",
    "\n",
    "\n",
    "def get_cosponsors(site_url):\n",
    "    url = site_url\n",
    "\n",
    "    # send GET request using selenium (sites in javascript)\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(' - incognito')\n",
    "    option.add_argument('--headless')\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path='/usr/local/bin/chromedriver', chrome_options=option)\n",
    "\n",
    "    req = requests.get(url)\n",
    "    \n",
    "    if req.status_code == 200:\n",
    "        soup = BeautifulSoup(req.content, 'lxml')\n",
    "        div = soup.find('div', {'id':'main'})\n",
    "        table = div.find('table')\n",
    "        rows = table.find_all('a')\n",
    "\n",
    "        empty_row = {\n",
    "            'cosponsor_name': None, \n",
    "            'cosponsor_party': None, \n",
    "            'cosponsor_state': None,\n",
    "            'cosponsor_district': None\n",
    "        }\n",
    "\n",
    "        all_rows = []\n",
    "\n",
    "        for row in rows:\n",
    "            new_row = copy.copy(empty_row)\n",
    "            rep = row.text.strip()\n",
    "\n",
    "            new_row['cosponsor_name'] = rep.rsplit('[', 1)[0][:-1]\n",
    "            party_dist = rep.rsplit('[', 1)[1][: -1]\n",
    "            party_dist_split = party_dist.split('-')\n",
    "            new_row['cosponsor_state'] = party_dist_split[0]\n",
    "            new_row['cosponsor_party'] = party_dist_split[1]\n",
    "            if len(party_dist_split) == 3:\n",
    "                new_row['cosponsor_district'] = party_dist_split[2][:-1]\n",
    "\n",
    "            all_rows.append(new_row)\n",
    "\n",
    "        return all_rows\n",
    "    \n",
    "    else:\n",
    "        output = {'url': url, 'error': req.status_code}\n",
    "        outpath = '../data/logs/cosponsor_errors.jsonl'\n",
    "        write_json_file(outpath, output)\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    # Set up Mongo for raw data and prettified data\n",
    "    client = MongoClient() # defaults to localhost\n",
    "    db = client.bills\n",
    "\n",
    "    # collection 'pages' is where the raw data is at\n",
    "    # collection 'bill_details' is where the prettified data will go \n",
    "    pages = db.pages\n",
    "\n",
    "    bill_details = db.bill_details\n",
    "\n",
    "    num_items = pages.find().count()\n",
    "    print('There are {} items in this collection.'.format(num_items))\n",
    "    \n",
    "    items = pages.find()\n",
    "\n",
    "    x = 0\n",
    "    \n",
    "    for i in items:\n",
    "        mongo_id = str(i['_id'])\n",
    "        mongo_log = '../data/logs/mongo_updates.jsonl'\n",
    "        soup = BeautifulSoup(i['lxml'], 'lxml')\n",
    "        soup_to_mongo(soup, bill_details)\n",
    "        write_json_file(mongo_id, mongo_log)\n",
    "\n",
    "        x += 1\n",
    "        if x%10 == 0:\n",
    "            print('Overall, {:.2f}% complete.'.format(100 * x / num_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.keys()\n",
    "soup = BeautifulSoup(item['lxml'], 'lxml')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>lxml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5c060fdccd68d14f1d062fa5</td>\n",
       "      <td>b'&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5c060fe4cd68d14f1d062fa6</td>\n",
       "      <td>b'&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5c060feccd68d14f1d062fa7</td>\n",
       "      <td>b'&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c060ff4cd68d14f1d062fa8</td>\n",
       "      <td>b'&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5c060ffccd68d14f1d062fa9</td>\n",
       "      <td>b'&lt;!DOCTYPE html&gt;\\n&lt;html class=\"no-js\" lang=\"e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id                                               lxml\n",
       "0  5c060fdccd68d14f1d062fa5  b'<!DOCTYPE html>\\n<html class=\"no-js\" lang=\"e...\n",
       "1  5c060fe4cd68d14f1d062fa6  b'<!DOCTYPE html>\\n<html class=\"no-js\" lang=\"e...\n",
       "2  5c060feccd68d14f1d062fa7  b'<!DOCTYPE html>\\n<html class=\"no-js\" lang=\"e...\n",
       "3  5c060ff4cd68d14f1d062fa8  b'<!DOCTYPE html>\\n<html class=\"no-js\" lang=\"e...\n",
       "4  5c060ffccd68d14f1d062fa9  b'<!DOCTYPE html>\\n<html class=\"no-js\" lang=\"e..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "items = pages.find()\n",
    "items_df = pd.DataFrame(list(items))\n",
    "items_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1011, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1011"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 250 rows to iterate through on this pass.\n",
      "8.00% complete\n",
      "16.00% complete\n",
      "24.00% complete\n",
      "32.00% complete\n",
      "40.00% complete\n",
      "48.00% complete\n",
      "56.00% complete\n",
      "64.00% complete\n",
      "72.00% complete\n",
      "80.00% complete\n",
      "88.00% complete\n",
      "96.00% complete\n"
     ]
    }
   ],
   "source": [
    "bill_listing = soup_to_jsonl(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leg_id': 'H.R.7211',\n",
       " 'leg_type': 'BILL',\n",
       " 'leg_url': 'https://www.congress.gov/bill/115th-congress/house-bill/7211?r=1',\n",
       " 'intro_date': '11/30/2018',\n",
       " 'congress_id': '115th',\n",
       " 'desc': 'To amend the Mineral Leasing Act to authorize the Secretary of the Interior to regulate hydraulic fracturing operations on Federal lands, and for other purposes.',\n",
       " 'sponsor': 'Rep. Soto, Darren',\n",
       " 'sponsor_party': 'FL',\n",
       " 'sponsor_state': 'D',\n",
       " 'sponsor_district': '9',\n",
       " 'num_of_cosponsors': '0',\n",
       " 'cosponsors_url': 'https://www.congress.gov/bill/115th-congress/house-bill/7211/cosponsors?r=1&overview=closed#tabs',\n",
       " 'cosponsors': None,\n",
       " 'committee': 'House - Natural Resources, Energy and Commerce',\n",
       " 'bill_status': 'Introduced',\n",
       " 'body': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bill_listing[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H R 7211'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 'H.R.7211'\n",
    "m = l.replace('.', ' ')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
