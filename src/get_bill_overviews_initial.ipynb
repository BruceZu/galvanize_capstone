{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_bill_overviews_initial.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_bill_overviews_initial.py\n",
    "'''\n",
    "---------------------------------------------------------\n",
    "This script is for the initial data pull of data overview infomation from Congress.gov and put into a Mongo\n",
    "collection. At the time of the initial pull for this project, 444 pages needed to be scraped to get every \n",
    "bill and joint resolution between the 110th and 115th Congress.\n",
    "\n",
    "Once information from the script is loaded into Mongo, run ../new_data/get_bill_text.py and \n",
    "../new_data/get_amendment_count.py to populate that info into the existing records\n",
    "---------------------------------------------------------\n",
    "'''\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import copy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import threading\n",
    "\n",
    "def get_soup(url):\n",
    "    '''\n",
    "    Get soup object from url to be parsed out in another function. If status code != 200, \n",
    "    prints out error message.\n",
    "    \n",
    "    Parameters: url\n",
    "    \n",
    "    Returns: BeautifulSoup object\n",
    "    '''\n",
    "    # included sleep time to attempt human user mimicking\n",
    "    sleep_time = randint(0, 6)\n",
    "    sleep(sleep_time)\n",
    "    req = requests.get(url)\n",
    "    stat_code = req.status_code\n",
    "\n",
    "    if stat_code != 200:\n",
    "        print('_______________')\n",
    "        print('_______________')\n",
    "        print('Error requesting {}'.format(url))\n",
    "        print('Request Status Code: {}'.format(stat_code))\n",
    "\n",
    "    if stat_code == 200:            \n",
    "        print('_______________')\n",
    "        print('_______________')\n",
    "        print('\\tRetrieving soup from {}'.format(url))\n",
    "        soup = BeautifulSoup(req.content, 'lxml')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "\n",
    "def soup_details_to_mongo(soup, collection):\n",
    "    '''\n",
    "    Parses out the details from the soup object and inserts the details into \n",
    "    Mongo database collection row by row.\n",
    "    \n",
    "    Parameters: soup - a soup object with table within 'ol' class\n",
    "                collection - collection name of Mongo database\n",
    "                \n",
    "    Returns:    None\n",
    "    '''\n",
    "    # initialize empty row to populate data\n",
    "    empty_row = {'leg_id': None, \n",
    "                'leg_type': None,\n",
    "                'leg_url': None,\n",
    "                'intro_date': None,\n",
    "                'congress_id': None,\n",
    "                'desc': None,\n",
    "                'sponsor': None, \n",
    "                'sponsor_party': None, \n",
    "                'sponsor_state': None,\n",
    "                'sponsor_district': None,  #senators don't have districts\n",
    "                'num_of_cosponsors': None,\n",
    "                'cosponsors_url': None,\n",
    "                'cosponsors': None,        #requires navigation to another url and extracting names from table\n",
    "                'num_of_amendments': None,  #requires navigation to another url\n",
    "                'committee': None, \n",
    "                'bill_status': None,\n",
    "                'body': None               #requires navigation to another url\n",
    "                }\n",
    "\n",
    "\n",
    "    # table of bills are in ol class\n",
    "    div = soup.find('div', {'class':'search-column-main'})\n",
    "    table = div.find('ol')\n",
    "\n",
    "    # iterate though each li class expanded to get rows\n",
    "    rows = table.find_all('li', {'class':'expanded'})\n",
    "   \n",
    "    for row in rows:\n",
    "        new_row = copy.copy(empty_row)\n",
    "\n",
    "        # parse items within 'span' tag\n",
    "        columns = row.find_all('span')\n",
    "        if len(columns) > 3:\n",
    "            # we only want bills and joint resolutions\n",
    "            legislation_type = columns[0].text.strip()\n",
    "\n",
    "            if (legislation_type == 'BILL') |  (legislation_type == 'JOINT RESOLUTION') | (legislation_type == 'LAW'):\n",
    "                if columns[0].text != '':\n",
    "                    new_row['leg_type'] = legislation_type\n",
    "                if columns[1].text.strip().split()[2] != '':\n",
    "                    new_row['congress_id'] = columns[1].text.strip().split()[2][:3]\n",
    "                if columns[2].text != '':\n",
    "                    new_row['desc'] = columns[2].text\n",
    "                if ('Committee' in columns[4].text):\n",
    "                    new_row['committee'] = columns[4].text.strip()[12:]\n",
    "\n",
    "                dt = columns[3].text.strip().split()\n",
    "                if '(Introduced' in dt:\n",
    "                    new_row['intro_date'] = dt[dt.index('(Introduced') + 1][:-1]\n",
    "\n",
    "\n",
    "                # bill_status is within 'p' tag\n",
    "                columns = row.find_all('p')\n",
    "                if columns[0].text.strip()[25:] != '':\n",
    "                    new_row['bill_status'] = columns[0].text.strip()[25:]\n",
    "\n",
    "\n",
    "                # parse info within 'a' tag\n",
    "                columns = row.find_all('a')\n",
    "                if columns[0].text.strip() != '':\n",
    "                    new_row['leg_id'] = columns[0].text.strip().replace('.', ' ')\n",
    "\n",
    "                # also within 'a' tag, reserved bill numbers will not have the information below\n",
    "                if (len(columns) > 2):    \n",
    "                    if columns[0]['href'].strip() != '':\n",
    "                        new_row['leg_url'] = columns[0]['href'].strip()\n",
    "                    if columns[2].text.strip() != '':\n",
    "                        new_row['num_of_cosponsors'] = columns[2].text.strip()\n",
    "                        if new_row['num_of_cosponsors'] != '0':\n",
    "                            new_row['cosponsors_url'] = columns[2]['href']\n",
    "\n",
    "                # party, state, and district (for house reps) need to be stripped out of sponsor info\n",
    "                    for c in range(len(columns)):\n",
    "                        if '[' in columns[c].text.strip():\n",
    "                            rep = columns[c].text.strip()\n",
    "                            new_row['sponsor'] = rep.rsplit('[', 1)[0][:-1][5:]\n",
    "                            party_dist = rep.rsplit('[', 1)[1][: -1]\n",
    "                            party_dist_split = party_dist.split('-')\n",
    "                            new_row['sponsor_state'] = party_dist_split[1]\n",
    "                            new_row['sponsor_party'] = party_dist_split[0]\n",
    "                            if len(party_dist_split) == 3:\n",
    "                                new_row['sponsor_district'] = party_dist_split[2]\n",
    "            \n",
    "                collection.insert_one(new_row)\n",
    "\n",
    "\n",
    "            \n",
    "def get_amendment_count(url):\n",
    "    '''\n",
    "    Returns amendment counts for a bill at the url\n",
    "    \n",
    "    Parameters: url - url that gives access to bill details\n",
    "    \n",
    "    Return: Integer - count of amendments\n",
    "    '''\n",
    "    soup = get_soup(url)\n",
    "    \n",
    "    # iterate through tabs to find Amendments and get count\n",
    "    tabs = soup.find('nav', {'id': 'tabs'})\n",
    "    info = tabs.find_all('a')\n",
    "    for i in info:\n",
    "        if 'Amendment' in i.text.split()[0]:        \n",
    "            return i.text.split()[1].strip('()')\n",
    "        \n",
    "        \n",
    "        \n",
    "def initiate_process(page):\n",
    "    client = MongoClient()\n",
    "    db = client.bills\n",
    "    bill_info = db.bill_info\n",
    "\n",
    "    url_root = 'https://www.congress.gov/search?q=%7B%22source%22%3A%22legislation%22%7D&pageSize=250&page='\n",
    "    \n",
    "    site_url = '{}{}'.format(url_root, page)\n",
    "\n",
    "#     print(site_url)\n",
    "    soup = get_soup(site_url)\n",
    "    soup_details_to_mongo(soup, bill_info)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # begin by populating Mongo with general info for bills and joint resolutions using threading\n",
    "\n",
    "    # the 110th Congress ends on page 444 with 250 results on page\n",
    "    # https://www.congress.gov/search?q=%7B%22source%22%3A%22legislation%22%7D&pageSize=250&page=2\n",
    "    page_range = range(1, 445)\n",
    "\n",
    "    for p in page_range[::4]:\n",
    "        t1 = threading.Thread(target=initiate_process, args=[p])\n",
    "        t2 = threading.Thread(target=initiate_process, args=[p+1])\n",
    "        t3 = threading.Thread(target=initiate_process, args=[p+2])\n",
    "        t4 = threading.Thread(target=initiate_process, args=[p+3])\n",
    "\n",
    "        t1.start()\n",
    "        t2.start()\n",
    "        t3.start()\n",
    "        t4.start()\n",
    "\n",
    "        t1.join()\n",
    "        t2.join()\n",
    "        t3.join()\n",
    "        t4.join()\n",
    "        \n",
    "    print('-----------')\n",
    "    print('-----------')\n",
    "    print('Initial data collection complete!... DATA SCIENCE!!!')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # once mongo data is populated, retrieve data from mongo to fill in additional details\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
