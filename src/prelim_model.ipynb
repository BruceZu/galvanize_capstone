{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile prelim_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams, skipgrams\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Number of records in database: 253000\n",
      "--> Current number of records with text: 16437\n",
      "\tCount for each bill_status: \n",
      "Introduced: \t\t10559\n",
      "Became Law: \t\t1529\n",
      "Passed House: \t\t2162\n",
      "To President: \t\t26\n",
      "Resolving Differences: \t\t72\n",
      "Failed House: \t\t114\n",
      "Became Private Law: \t\t2\n",
      "Passed Senate: \t\t163\n",
      "Failed Senate: \t\t2\n",
      "Failed to pass over veto: \t\t34\n",
      "Vetoed by President: \t\t27\n",
      "Passed over veto: \t\t1\n",
      "Pocket vetoed by President: \t\t5\n",
      "Shape of entire data before labeling: (14696, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of entire data after labeling: (14689, 18)\n",
      "----------------\n",
      "\tData loss occurred during labeling. You may want to examine your code\n",
      "in_progress    10779\n",
      "0               2341\n",
      "1               1562\n",
      "Name: passed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# connect to mongodb\n",
    "client = MongoClient() # defaults to localhost\n",
    "db = client.bills\n",
    "bill_details = db.bill_details\n",
    "\n",
    "\n",
    "# print out record counts with text\n",
    "print('--> Number of records in database: {}'.format(bill_details.find().count()))\n",
    "\n",
    "records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "record_count = records_with_text.count()\n",
    "\n",
    "print('--> Current number of records with text: {}'.format(record_count))\n",
    "\n",
    "\n",
    "# convert mongo query resuls to dataframe\n",
    "# need to execute query (.find) everytime i refer to it?\n",
    "records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "data = pd.DataFrame(list(records_with_text))\n",
    "\n",
    "# filter out simple resolutions, concurrent resolutions, and amendments (for prelim model)\n",
    "data = data[(data['leg_type'] != 'RESOLUTION') & (data['leg_type'] != 'CONCURRENT RESOLUTION') & (data['leg_type'] != 'AMENDMENT')]\n",
    "\n",
    "\n",
    "# LABELS\n",
    "\n",
    "# Every record that doesn't have status Became Law will have label 0 if before current (115th) congress.\n",
    "# plan is to use one label ('passed') initially.\n",
    "# Try this out with 3 labels.\n",
    "\n",
    "#  \n",
    "\n",
    "#                             Whole     House     Senate\n",
    "# Introduced:                 None      None      None\n",
    "# Became Law:                 1         1         1\n",
    "# Passed House:               None      1         None\n",
    "# To President:               1         1         1\n",
    "# Resolving Differences:      1         1         1\n",
    "# Failed House:               0         0         1 if S\n",
    "# Became Private Law:         1         1         1\n",
    "# Passed Senate:              None      None      1\n",
    "# Failed to pass over veto:   1         1         1\n",
    "# Vetoed by President:        1         1         1\n",
    "# Passed over veto:           1         1         1     #stronger support for this one???\n",
    "# Pocket vetoed by President: 1         1         1\n",
    "# Failed Senate:              0         1 if H    0\n",
    "\n",
    "\n",
    "# check numbers for each status\n",
    "print('-----------------')\n",
    "print('-----------------')\n",
    "print('\\tCount for each bill_status: ')\n",
    "for i in data.bill_status.unique():\n",
    "    num = len(data[data['bill_status'] == i])\n",
    "    print('{}: \\t\\t{}'.format(i, num))\n",
    "\n",
    "# create columns for labels\n",
    "# data['house_label'] = None\n",
    "# data['senate_label'] = None\n",
    "# data['president_label'] = None\n",
    "data['passed'] = None\n",
    "\n",
    "orig_shape = data.shape\n",
    "print('Shape of entire data before labeling: {}'.format(orig_shape))\n",
    "\n",
    "\n",
    "# break up dataframe into those that became law and others (did not or still pending)\n",
    "became_law = data[(data['bill_status'] == 'Became Law') | (data['bill_status'] == 'Became Private Law')]\n",
    "others = data[(data['bill_status'] != 'Became Law') & (data['bill_status'] != 'Became Private Law')]\n",
    "\n",
    "became_law.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# break up others into current congress and previous ones. Anything that hasn't been signed into law\n",
    "# before current session is dead. Currently, all bills vetoed by the president come from previous congresses\n",
    "current_cong = others[others['congress_id'] == '115th']\n",
    "prev_cong = others[others['congress_id'] != '115th']\n",
    "\n",
    "prev_cong.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# let's label To President and Resolving Differences with 1. Everything else is on the floor\n",
    "to_pres = current_cong[(current_cong['bill_status'] == 'To President') | (current_cong['bill_status'] == 'Resolving Differences')]\n",
    "on_floor = current_cong[(current_cong['bill_status'] != 'To President') & (current_cong['bill_status'] != 'Resolving Differences')]\n",
    "\n",
    "to_pres.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# break up bills on the floor to failed (0) and not failed\n",
    "failed = on_floor[on_floor['bill_status'].str.startswith('Failed')]\n",
    "not_failed = on_floor[~on_floor['bill_status'].str.startswith('Failed')]\n",
    "\n",
    "failed.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# bills that haven't failed yet have either been just introduced or on their way\n",
    "# label introduced with 'in_progress'. These will not be a part of our model.\n",
    "introduced = not_failed[not_failed['bill_status'] == 'Introduced']\n",
    "beyond_intro = not_failed[not_failed['bill_status'] != 'Introduced']\n",
    "\n",
    "introduced.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "# there are bills that started in one chamber and have already passed the other. We'll label\n",
    "# these with a 1\n",
    "passed_opp_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('S')) | \n",
    "                          (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('H'))]\n",
    "\n",
    "passed_opp_chamber.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# bills that are still in the chamber they were introduced in are 'in_progress'\n",
    "in_orig_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('H')) | \n",
    "                          (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('S'))]    \n",
    "\n",
    "in_orig_chamber.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "# bring all the information back together\n",
    "data_l = pd.concat([became_law, prev_cong, to_pres, failed, introduced, in_opp_chamber, in_orig_chamber])\n",
    "\n",
    "labeled_shape = data_l.shape\n",
    "print('Shape of entire data after labeling: {}'.format(labeled_shape))\n",
    "print('----------------')\n",
    "\n",
    "if orig_shape == labeled_shape:\n",
    "    print('\\tNo data loss upon labeling. Continue on your path, Barsen\\'thor.')\n",
    "else:\n",
    "    print('\\tData loss occurred during labeling. You may want to examine your code')    \n",
    "\n",
    "\n",
    "print(data_l.passed.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_l[data_l['passed'] != 'in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'bill_status', 'body', 'committee', 'congress_id', 'cosponsors',\n",
       "       'cosponsors_url', 'desc', 'intro_date', 'leg_id', 'leg_type', 'leg_url',\n",
       "       'num_of_cosponsors', 'sponsor', 'sponsor_district', 'sponsor_party',\n",
       "       'sponsor_state', 'passed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Created corpus, tokenizing...\n",
      "------------------\n",
      "Stripping out stop words and punctuation...\n",
      "------------------\n",
      "Lemmatizing...\n",
      "------------------\n",
      "Creating a vocabulary...\n"
     ]
    }
   ],
   "source": [
    "# on initial pass, trying nlp on bill text\n",
    "# create a corpus\n",
    "documents = list(df['body'])\n",
    "\n",
    "# tokenize the corpus\n",
    "print('------------------')\n",
    "print('Created corpus, now tokenizing it...')\n",
    "corpus = [word_tokenize(content.lower()) for content in documents]\n",
    "\n",
    "# strip out the stop words from each \n",
    "print('------------------')\n",
    "print('Stripping out stop words and punctuation...')\n",
    "stop_words = stopwords.words('english')\n",
    "# print(stop_words)\n",
    "corpus = [[token for token in doc if token not in stop_words] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# strip out the punctuation\n",
    "punc = set(string.punctuation)\n",
    "# print(punc)\n",
    "corpus = [[token for token in doc if token not in punc] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# lemmatize (and maybe stem)\n",
    "print('------------------')\n",
    "print('Lemmatizing...')\n",
    "lemmer = WordNetLemmatizer()\n",
    "corpus = [[lemmer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# build a vocabulary\n",
    "print('------------------')\n",
    "print('Creating a vocabulary...')\n",
    "vocab_set = set()\n",
    "[[vocab_set.add(token) for token in tokens] for tokens in corpus]\n",
    "vocab = list(vocab_set)\n",
    "# vocab[100000:100020]\n",
    "\n",
    "# # for later model...\n",
    "# # examine n-grams...\n",
    "# # bigrams (two words side-by-side)\n",
    "# print('------------------')\n",
    "# print('Creating lists of bigrams, trigrams, skipgrams, etc...')\n",
    "# bigrams = [list(ngrams(sequence = doc, n = 2)) for doc in corpus]\n",
    "# trigrams = [list(ngrams(sequence = doc, n = 3)) for doc in corpus]\n",
    "# #... more?\n",
    "\n",
    "# # skipgrams (n-grams that skip k words)\n",
    "# skipgrams = [list(skipgrams(sequence = doc, n = 2, k = 1)) for doc in corpus]\n",
    "\n",
    "\n",
    "# rejoin each doc in corpus so each doc is a single string\n",
    "corpus = [' '.join(tokens) for tokens in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on initial pass, trying nlp on bill text\n",
    "X = corpus\n",
    "y = df['passed']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bag of words. CountVectorizer allows us to build a term frequency matrix\n",
    "print('------------------')\n",
    "print('Using CountVectorizer to create a term frequency matrix...')\n",
    "cv = CountVectorizer(stop_words = 'english')\n",
    "bag_of_words = cv.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature dictionary with indices\n",
    "# The same can be done with tfidfvectorizer\n",
    "print('------------------')\n",
    "print('Creating vocabulary and feature list...')\n",
    "feature_dict = bag_of_words.vocabulary_\n",
    "# print(feature_dict)\n",
    "\n",
    "# create an alphabetical feature list\n",
    "feature_list = bag_of_words.get_feature_names()\n",
    "# print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a term frequency matrix. This is a sparse array. \n",
    "term_freq_matrix = cv.fit_transform(corpus).toarray()\n",
    "# print(term_freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3910, 83785)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-187-4435444f5a91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_test_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(norm = 'l2')\n",
    "X_train_vec = tfvect.fit_transform(X_train)\n",
    "X_test_vec = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred = nb.predict(X_test_vec)\n",
    "\n",
    "print('--------------------')\n",
    "print('--------------------')\n",
    "print('Multinomial Naive Bayes predictions after TFIDFVectorizer')\n",
    "print('\\tAccuracy Score: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('\\tRecall Score: {}'.format(recall_score(y_test, y_pred)))\n",
    "print('\\tPrecision Score: {}'.format(precision_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(vectorizer, vectors, data, words, n):\n",
    "    '''\n",
    "    Print out the top 10 words by three different sorting mechanisms:\n",
    "        * average tf-idf score\n",
    "        * total tf-idf score\n",
    "        * highest TF score across corpus\n",
    "    '''\n",
    "#     words = vectorizer.get_feature_names()\n",
    "\n",
    "    # Top 10 words by average tfidf\n",
    "    # Take the average of each column, denoted by axis=0\n",
    "    avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "    print (\"top %d by average tf-idf\" % n)\n",
    "    print (get_top_values(avg, n, words))\n",
    "    print ('------------------------')\n",
    "\n",
    "    # Top 10 words by total tfidf\n",
    "    total = np.sum(vectors, axis=0)\n",
    "    print (\"top %d by total tf-idf\" % n)\n",
    "    print (get_top_values(total, n, words))\n",
    "    print ('------------------------')\n",
    "\n",
    "    # Top 10 words by TF\n",
    "    vectorizer2 = TfidfVectorizer(use_idf=False)\n",
    "    # make documents into one giant document for this purpose\n",
    "    vectors2 = vectorizer2.fit_transform([\" \".join(data)]).toarray()\n",
    "    print (\"top %d by tf across all corpus\" % n)\n",
    "    print (get_top_values(vectors2[0], n, words))\n",
    "    print ('------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values. Return\n",
    "    the labels for each of these indices.\n",
    "\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 by average tf-idf\n",
      "['tolono', 'chlorobenzophenone', 'caig', 'lactating', 'dropsonde', 'bridged', 'terans', 'udc', 'lengthy', 'blythe', 'misappropriate', 'dianna', 'idot', 'propel', 'archived', 'majeur', 'pendency', 'showed', 'atomics', 'fundraise', 'bothell', 'sealions', 'midtown', 'saponi', 'mai', 'nominates', 'pappa', 'sphere', 'sealable', 'guamanian', 'batelle', 'supercomputer', 'lyster', '5350', 'kl', 'midservice', 'keyport', 'inspiration', 'husband', 'genitalia', 'chattanooga', 'alma', '6186', 'ncrar', 'differentiating', 'kaktovik', 'mechanicsburg', 'mccormick', 'sighted', 'ashlynne']\n",
      "------------------------\n",
      "top 50 by total tf-idf\n",
      "['semimanufactured', 'shoah', 'acti', 'steviol', 'semester', '000', 'unsecure', 'suitland', 'selectman', 'proper', 'zoar', 'partners', 'shaped', 'torial', 'mcgrath', 'conomy', 'fvpsa', 'fencing', 'instruments', 'ameneded', 'leasing', 'subection', 'puyallup', 'nter', 'amplifier', 'agents', 'ndi', 'huffman', 'publ032', 'geoplatform', 'averted', 'dcapes', 'flared', 'cog', 'fore', 'thames', 'resear', 'multiforum', 'qadhafi', 'senatorial', 'helena', 'servicin', 'comonomer', 'imethyl', 'majestic', 'indemnity', 'deport', 'communism', 'proposals', 'initio']\n",
      "------------------------\n",
      "top 50 by tf across all corpus\n",
      "['semimanufactured', 'shoah', 'steviol', 'acti', 'selectman', 'semester', '000', 'suitland', 'zoar', 'proper', 'mcgrath', 'unsecure', 'partners', 'torial', 'shaped', 'fvpsa', 'ameneded', 'publ032', 'instruments', 'averted', 'amplifier', 'fencing', 'geoplatform', 'subection', 'agents', 'fore', 'imethyl', 'flared', 'dcapes', 'leasing', 'indemnity', 'majestic', 'ndi', 'puyallup', 'thames', 'resear', 'qadhafi', 'deport', 'deforested', 'pmeasurement', 'nter', 'assumed', 'proposals', 'actor', 'comonomer', 'suchclearing', 'initio', 'automatically', 'senatorial', 'cog']\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "top_n(tf, tfvectors, corpus, feature_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # an option for large set\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=10)\n",
    "# features = hv.transform(corpus)\n",
    "# print(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
