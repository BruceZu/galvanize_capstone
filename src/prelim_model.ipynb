{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile prelim_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams, skipgrams\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Number of records in database: 253000\n",
      "--> Current number of records with text: 22458\n"
     ]
    }
   ],
   "source": [
    "# connect to mongodb\n",
    "client = MongoClient() # defaults to localhost\n",
    "db = client.bills\n",
    "bill_details = db.bill_details\n",
    "\n",
    "\n",
    "# print out record counts with text\n",
    "print('--> Number of records in database: {}'.format(bill_details.find().count()))\n",
    "\n",
    "records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "record_count = records_with_text.count()\n",
    "\n",
    "print('--> Current number of records with text: {}'.format(record_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "-----------------\n",
      "\tCount for each bill_status: \n",
      "Introduced: \t\t16188\n",
      "Became Law: \t\t1662\n",
      "Passed House: \t\t2451\n",
      "To President: \t\t26\n",
      "Resolving Differences: \t\t72\n",
      "Failed House: \t\t115\n",
      "Became Private Law: \t\t2\n",
      "Passed Senate: \t\t171\n",
      "Failed Senate: \t\t2\n",
      "Failed to pass over veto: \t\t35\n",
      "Vetoed by President: \t\t28\n",
      "Passed over veto: \t\t1\n",
      "Pocket vetoed by President: \t\t5\n",
      "Shape of entire data before labeling: (20758, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of entire data after labeling: (20758, 18)\n",
      "----------------\n",
      "\tNo data loss upon labeling. Continue on your path, Barsen'thor.\n",
      "in_progress    10779\n",
      "0               8270\n",
      "1               1709\n",
      "Name: passed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert mongo query resuls to dataframe\n",
    "# need to execute query (.find) everytime i refer to it?\n",
    "records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "data = pd.DataFrame(list(records_with_text))\n",
    "\n",
    "# filter out simple resolutions, concurrent resolutions, and amendments (for prelim model)\n",
    "data = data[(data['leg_type'] != 'RESOLUTION') & (data['leg_type'] != 'CONCURRENT RESOLUTION') & (data['leg_type'] != 'AMENDMENT')]\n",
    "\n",
    "\n",
    "# LABELS\n",
    "\n",
    "# Every record that doesn't have status Became Law will have label 0 if before current (115th) congress.\n",
    "# plan is to use one label ('passed') initially.\n",
    "# Try this out with 3 labels.\n",
    "\n",
    "#  \n",
    "\n",
    "#                             Whole     House     Senate\n",
    "# Introduced:                 None      None      None\n",
    "# Became Law:                 1         1         1\n",
    "# Passed House:               None      1         None\n",
    "# To President:               1         1         1\n",
    "# Resolving Differences:      1         1         1\n",
    "# Failed House:               0         0         1 if S\n",
    "# Became Private Law:         1         1         1\n",
    "# Passed Senate:              None      None      1\n",
    "# Failed to pass over veto:   1         1         1\n",
    "# Vetoed by President:        1         1         1\n",
    "# Passed over veto:           1         1         1     #stronger support for this one???\n",
    "# Pocket vetoed by President: 1         1         1\n",
    "# Failed Senate:              0         1 if H    0\n",
    "\n",
    "\n",
    "# check numbers for each status\n",
    "print('-----------------')\n",
    "print('-----------------')\n",
    "print('\\tCount for each bill_status: ')\n",
    "for i in data.bill_status.unique():\n",
    "    num = len(data[data['bill_status'] == i])\n",
    "    print('{}: \\t\\t{}'.format(i, num))\n",
    "\n",
    "# create columns for labels\n",
    "# data['house_label'] = None\n",
    "# data['senate_label'] = None\n",
    "# data['president_label'] = None\n",
    "data['passed'] = None\n",
    "\n",
    "orig_shape = data.shape\n",
    "print('Shape of entire data before labeling: {}'.format(orig_shape))\n",
    "\n",
    "\n",
    "# break up dataframe into those that became law and others (did not or still pending)\n",
    "became_law = data[(data['bill_status'] == 'Became Law') | (data['bill_status'] == 'Became Private Law')]\n",
    "others = data[(data['bill_status'] != 'Became Law') & (data['bill_status'] != 'Became Private Law')]\n",
    "\n",
    "became_law.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# break up others into current congress and previous ones. Anything that hasn't been signed into law\n",
    "# before current session is dead. Currently, all bills vetoed by the president come from previous congresses\n",
    "current_cong = others[others['congress_id'] == '115th']\n",
    "prev_cong = others[others['congress_id'] != '115th']\n",
    "\n",
    "prev_cong.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# let's label To President and Resolving Differences with 1. Everything else is on the floor\n",
    "to_pres = current_cong[(current_cong['bill_status'] == 'To President') | (current_cong['bill_status'] == 'Resolving Differences')]\n",
    "on_floor = current_cong[(current_cong['bill_status'] != 'To President') & (current_cong['bill_status'] != 'Resolving Differences')]\n",
    "\n",
    "to_pres.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# break up bills on the floor to failed (0) and not failed\n",
    "failed = on_floor[on_floor['bill_status'].str.startswith('Failed')]\n",
    "not_failed = on_floor[~on_floor['bill_status'].str.startswith('Failed')]\n",
    "\n",
    "failed.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# bills that haven't failed yet have either been just introduced or on their way\n",
    "# label introduced with 'in_progress'. These will not be a part of our model.\n",
    "introduced = not_failed[not_failed['bill_status'] == 'Introduced']\n",
    "beyond_intro = not_failed[not_failed['bill_status'] != 'Introduced']\n",
    "\n",
    "introduced.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "# there are bills that started in one chamber and have already passed the other. We'll label\n",
    "# these with a 1\n",
    "passed_opp_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('S')) | \n",
    "                          (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('H'))]\n",
    "\n",
    "passed_opp_chamber.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# bills that are still in the chamber they were introduced in are 'in_progress'\n",
    "in_orig_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('H')) | \n",
    "                          (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('S'))]    \n",
    "\n",
    "in_orig_chamber.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "# bring all the information back together\n",
    "data_l = pd.concat([became_law, prev_cong, to_pres, failed, introduced, passed_opp_chamber, in_orig_chamber])\n",
    "\n",
    "labeled_shape = data_l.shape\n",
    "print('Shape of entire data after labeling: {}'.format(labeled_shape))\n",
    "print('----------------')\n",
    "\n",
    "if orig_shape == labeled_shape:\n",
    "    print('\\tNo data loss upon labeling. Continue on your path, Barsen\\'thor.')\n",
    "else:\n",
    "    print('\\tYou lost {} lines upon labeling. You may want to examine your code.'.format(orig_shape[0] - labeled_shape[0]))\n",
    "\n",
    "\n",
    "print(data_l.passed.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_l[data_l['passed'] != 'in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_id', 'bill_status', 'body', 'committee', 'congress_id', 'cosponsors',\n",
       "       'cosponsors_url', 'desc', 'intro_date', 'leg_id', 'leg_type', 'leg_url',\n",
       "       'num_of_cosponsors', 'sponsor', 'sponsor_district', 'sponsor_party',\n",
       "       'sponsor_state', 'passed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Created corpus, now tokenizing it...\n",
      "------------------\n",
      "Stripping out stop words and punctuation...\n",
      "------------------\n",
      "Lemmatizing...\n",
      "------------------\n",
      "Creating a vocabulary...\n",
      "------------------\n",
      "NLP preprocessing complete ...\n"
     ]
    }
   ],
   "source": [
    "# on initial pass, trying nlp on bill text\n",
    "# create a corpus\n",
    "documents = list(df['body'])\n",
    "\n",
    "# tokenize the corpus\n",
    "print('------------------')\n",
    "print('Created corpus, now tokenizing it...')\n",
    "corpus = [word_tokenize(content.lower()) for content in documents]\n",
    "\n",
    "# strip out the stop words from each \n",
    "print('------------------')\n",
    "print('Stripping out stop words and punctuation...')\n",
    "stop_words = stopwords.words('english')\n",
    "# print(stop_words)\n",
    "corpus = [[token for token in doc if token not in stop_words] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# strip out the punctuation\n",
    "punc = set(string.punctuation)\n",
    "# print(punc)\n",
    "corpus = [[token for token in doc if token not in punc] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# lemmatize (and maybe stem)\n",
    "print('------------------')\n",
    "print('Lemmatizing...')\n",
    "lemmer = WordNetLemmatizer()\n",
    "corpus = [[lemmer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# build a vocabulary\n",
    "print('------------------')\n",
    "print('Creating a vocabulary...')\n",
    "vocab_set = set()\n",
    "[[vocab_set.add(token) for token in tokens] for tokens in corpus]\n",
    "vocab = list(vocab_set)\n",
    "# vocab[100000:100020]\n",
    "\n",
    "# # for later model...\n",
    "# # examine n-grams...\n",
    "# # bigrams (two words side-by-side)\n",
    "# print('------------------')\n",
    "# print('Creating lists of bigrams, trigrams, skipgrams, etc...')\n",
    "# bigrams = [list(ngrams(sequence = doc, n = 2)) for doc in corpus]\n",
    "# trigrams = [list(ngrams(sequence = doc, n = 3)) for doc in corpus]\n",
    "# #... more?\n",
    "\n",
    "# # skipgrams (n-grams that skip k words)\n",
    "# skipgrams = [list(skipgrams(sequence = doc, n = 2, k = 1)) for doc in corpus]\n",
    "\n",
    "\n",
    "# rejoin each doc in corpus so each doc is a single string\n",
    "corpus = [' '.join(tokens) for tokens in corpus]\n",
    "\n",
    "print('------------------')\n",
    "print('NLP preprocessing complete ...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on initial pass, trying nlp on bill text\n",
    "X = corpus\n",
    "y = df['passed'].astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a bag of words. CountVectorizer allows us to build a term frequency matrix\n",
    "# print('------------------')\n",
    "# print('Using CountVectorizer to create a term frequency matrix...')\n",
    "# cv = CountVectorizer(stop_words = 'english')\n",
    "# bag_of_words = cv.fit(X_train)\n",
    "\n",
    "# # create a feature dictionary with indices\n",
    "# # The same can be done with tfidfvectorizer\n",
    "# print('------------------')\n",
    "# print('Creating vocabulary and feature list...')\n",
    "# feature_dict = bag_of_words.vocabulary_\n",
    "# # print(feature_dict)\n",
    "\n",
    "# # create an alphabetical feature list\n",
    "# feature_list = bag_of_words.get_feature_names()\n",
    "# # print(feature_list)\n",
    "\n",
    "# # convert to a term frequency matrix. This is a sparse array. \n",
    "# term_freq_matrix = cv.fit_transform(X_train).toarray()\n",
    "# # print(term_freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer\n",
    "tfvect = TfidfVectorizer()\n",
    "X_train_vec = tfvect.fit_transform(X_train)\n",
    "X_test_vec = tfvect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--------------------\n",
      "--> Multinomial Naive Bayes predictions after TFIDFVectorizer\n",
      "Accuracy Score: 0.8392785571142285\n",
      "Recall Score: 0.09070294784580499\n",
      "Precision Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[2054    0]\n",
      " [ 401   40]]\n",
      "--------------------\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred = nb.predict(X_test_vec).astype('int')\n",
    "\n",
    "print('--------------------')\n",
    "print('--------------------')\n",
    "print('--> Multinomial Naive Bayes predictions after TFIDFVectorizer')\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Recall Score: {}'.format(recall_score(y_test, y_pred)))\n",
    "print('Precision Score: {}'.format(precision_score(y_test, y_pred)))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred, labels = [0, 1]))\n",
    "print('--------------------')\n",
    "print('--------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(vectorizer, vectors, data, words, n):\n",
    "    '''\n",
    "    Print out the top 10 words by three different sorting mechanisms:\n",
    "        * average tf-idf score\n",
    "        * total tf-idf score\n",
    "        * highest TF score across corpus\n",
    "    '''\n",
    "#     words = vectorizer.get_feature_names()\n",
    "\n",
    "    # Top 10 words by average tfidf\n",
    "    # Take the average of each column, denoted by axis=0\n",
    "    avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "    print (\"top %d by average tf-idf\" % n)\n",
    "    print (get_top_values(avg, n, words))\n",
    "    print ('------------------------')\n",
    "\n",
    "    # Top 10 words by total tfidf\n",
    "    total = np.sum(vectors, axis=0)\n",
    "    print (\"top %d by total tf-idf\" % n)\n",
    "    print (get_top_values(total, n, words))\n",
    "    print ('------------------------')\n",
    "\n",
    "    # Top 10 words by TF\n",
    "    vectorizer2 = TfidfVectorizer(use_idf=False)\n",
    "    # make documents into one giant document for this purpose\n",
    "    vectors2 = vectorizer2.fit_transform([\" \".join(data)]).toarray()\n",
    "    print (\"top %d by tf across all corpus\" % n)\n",
    "    print (get_top_values(vectors2[0], n, words))\n",
    "    print ('------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values. Return\n",
    "    the labels for each of these indices.\n",
    "\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 by average tf-idf\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-bdaa2a557343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfvect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-ca8cccea27ae>\u001b[0m in \u001b[0;36mtop_n\u001b[0;34m(vectorizer, vectors, data, words, n)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"top %d by average tf-idf\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_top_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-287c96210d47>\u001b[0m in \u001b[0;36mget_top_values\u001b[0;34m(lst, n, labels)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-287c96210d47>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "top_n(tfvect, X_train_vec, corpus, feature_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # an option for large set\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=10)\n",
    "# features = hv.transform(corpus)\n",
    "# print(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
