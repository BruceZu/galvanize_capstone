{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile prelim_model.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, confusion_matrix\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams, skipgrams\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB#, ComplementNB unreleased as of 12/14\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  \n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Number of records in database: 253000\n",
      "--> Current number of records with text: 64676\n",
      "-----------------\n",
      "-----------------\n",
      "\tCount for each bill_status: \n",
      "Introduced: \t\t55166\n",
      "Became Law: \t\t2973\n",
      "Passed House: \t\t3964\n",
      "To President: \t\t26\n",
      "Resolving Differences: \t\t74\n",
      "Failed House: \t\t152\n",
      "Became Private Law: \t\t5\n",
      "Passed Senate: \t\t491\n",
      "Failed Senate: \t\t8\n",
      "Failed to pass over veto: \t\t37\n",
      "Vetoed by President: \t\t32\n",
      "Passed over veto: \t\t1\n",
      "Pocket vetoed by President: \t\t5\n",
      "-----------------\n",
      "-----------------\n",
      "Shape of entire data before labeling: (62934, 18)\n",
      "-----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Shape of entire data after labeling: (62934, 18)\n",
      "----------------\n",
      "\tNo data loss upon labeling. Onward!\n",
      "----------------\n",
      "----------------\n",
      "Count of records being examined for each label:\n",
      "0    49132\n",
      "1     3023\n",
      "Name: passed, dtype: int64\n",
      "----------------\n",
      "Value counts for each congress:\n",
      "111th    10769\n",
      "112th    10612\n",
      "114th    10223\n",
      "113th     9082\n",
      "110th     9022\n",
      "115th      331\n",
      "106th      321\n",
      "108th      293\n",
      "109th      264\n",
      "107th      249\n",
      "105th      245\n",
      "104th      214\n",
      "103rd      205\n",
      "102nd      203\n",
      "101st      122\n",
      "Name: congress_id, dtype: int64\n",
      "------------------\n",
      "Filtering for all congress_ids since the 110th Congress (2007) for analysis...\n"
     ]
    }
   ],
   "source": [
    "# connect to mongodb\n",
    "client = MongoClient() # defaults to localhost\n",
    "db = client.bills\n",
    "bill_details = db.bill_details\n",
    "\n",
    "\n",
    "# print out record counts with text\n",
    "print('--> Number of records in database: {}'.format(bill_details.find().count()))\n",
    "\n",
    "records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "record_count = records_with_text.count()\n",
    "\n",
    "print('--> Current number of records with text: {}'.format(record_count))\n",
    "\n",
    "# convert mongo query resuls to dataframe\n",
    "# need to execute query (.find) everytime i refer to it?\n",
    "records_with_text = bill_details.find({'body': {'$regex': 'e'}})\n",
    "data = pd.DataFrame(list(records_with_text))\n",
    "\n",
    "# filter out simple resolutions, concurrent resolutions, and amendments (for prelim model)\n",
    "data = data[(data['leg_type'] != 'RESOLUTION') & (data['leg_type'] != 'CONCURRENT RESOLUTION') & (data['leg_type'] != 'AMENDMENT')]\n",
    "\n",
    "\n",
    "# LABELS\n",
    "\n",
    "# Every record that doesn't have status Became Law will have label 0 if before current (115th) congress.\n",
    "# plan is to use one label ('passed') initially.\n",
    "# Try this out with 3 labels.\n",
    "\n",
    "#  \n",
    "\n",
    "#                             Whole     House     Senate\n",
    "# Introduced:                 None      None      None\n",
    "# Became Law:                 1         1         1\n",
    "# Passed House:               None      1         None\n",
    "# To President:               1         1         1\n",
    "# Resolving Differences:      1         1         1\n",
    "# Failed House:               0         0         1 if S\n",
    "# Became Private Law:         1         1         1\n",
    "# Passed Senate:              None      None      1\n",
    "# Failed to pass over veto:   1         1         1\n",
    "# Vetoed by President:        1         1         1\n",
    "# Passed over veto:           1         1         1     #stronger support for this one???\n",
    "# Pocket vetoed by President: 1         1         1\n",
    "# Failed Senate:              0         1 if H    0\n",
    "\n",
    "\n",
    "# check numbers for each status\n",
    "print('-----------------')\n",
    "print('-----------------')\n",
    "print('\\tCount for each bill_status: ')\n",
    "for i in data.bill_status.unique():\n",
    "    num = len(data[data['bill_status'] == i])\n",
    "    print('{}: \\t\\t{}'.format(i, num))\n",
    "\n",
    "# create columns for labels\n",
    "# data['house_label'] = None\n",
    "# data['senate_label'] = None\n",
    "# data['president_label'] = None\n",
    "data['passed'] = None\n",
    "\n",
    "orig_shape = data.shape\n",
    "print('-----------------')\n",
    "print('-----------------')\n",
    "print('Shape of entire data before labeling: {}'.format(orig_shape))\n",
    "print('-----------------')\n",
    "\n",
    "\n",
    "# break up dataframe into those that became law and others (did not or still pending)\n",
    "became_law = data[(data['bill_status'] == 'Became Law') | (data['bill_status'] == 'Became Private Law')]\n",
    "others = data[(data['bill_status'] != 'Became Law') & (data['bill_status'] != 'Became Private Law')]\n",
    "\n",
    "became_law.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# break up others into current congress and previous ones. Anything that hasn't been signed into law\n",
    "# before current session is dead. Currently, all bills vetoed by the president come from previous congresses\n",
    "current_cong = others[others['congress_id'] == '115th']\n",
    "prev_cong = others[others['congress_id'] != '115th']\n",
    "\n",
    "prev_cong.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# let's label To President and Resolving Differences with 1. Everything else is on the floor\n",
    "to_pres = current_cong[(current_cong['bill_status'] == 'To President') | (current_cong['bill_status'] == 'Resolving Differences')]\n",
    "on_floor = current_cong[(current_cong['bill_status'] != 'To President') & (current_cong['bill_status'] != 'Resolving Differences')]\n",
    "\n",
    "to_pres.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# break up bills on the floor to failed (0) and not failed\n",
    "failed = on_floor[on_floor['bill_status'].str.startswith('Failed')]\n",
    "not_failed = on_floor[~on_floor['bill_status'].str.startswith('Failed')]\n",
    "\n",
    "failed.loc[:, 'passed'] = 0\n",
    "\n",
    "\n",
    "\n",
    "# bills that haven't failed yet have either been just introduced or on their way\n",
    "# label introduced with 'in_progress'. These will not be a part of our model.\n",
    "introduced = not_failed[not_failed['bill_status'] == 'Introduced']\n",
    "beyond_intro = not_failed[not_failed['bill_status'] != 'Introduced']\n",
    "\n",
    "introduced.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "# there are bills that started in one chamber and have already passed the other. We'll label\n",
    "# these with a 1\n",
    "passed_opp_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('S')) | \n",
    "                          (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('H'))]\n",
    "\n",
    "passed_opp_chamber.loc[:, 'passed'] = 1\n",
    "\n",
    "\n",
    "\n",
    "# bills that are still in the chamber they were introduced in are 'in_progress'\n",
    "in_orig_chamber = beyond_intro[(beyond_intro['bill_status'] == 'Passed House') & (beyond_intro['leg_id'].str.startswith('H')) | \n",
    "                          (beyond_intro['bill_status'] == 'Passed Senate') & (beyond_intro['leg_id'].str.startswith('S'))]    \n",
    "\n",
    "in_orig_chamber.loc[:, 'passed'] = 'in_progress'\n",
    "\n",
    "\n",
    "\n",
    "# bring all the information back together\n",
    "data_l = pd.concat([became_law, prev_cong, to_pres, failed, introduced, passed_opp_chamber, in_orig_chamber])\n",
    "\n",
    "labeled_shape = data_l.shape\n",
    "print('----------------')\n",
    "print('Shape of entire data after labeling: {}'.format(labeled_shape))\n",
    "print('----------------')\n",
    "\n",
    "if orig_shape == labeled_shape:\n",
    "    print('\\tNo data loss upon labeling. Onward!')\n",
    "else:\n",
    "    print('\\tYou lost {} lines upon labeling. You may want to examine your code.'.format(orig_shape[0] - labeled_shape[0]))\n",
    "\n",
    "\n",
    "# filter out bills with label 'in_progress'\n",
    "print('----------------')\n",
    "print('----------------')\n",
    "df = data_l[data_l['passed'] != 'in_progress']\n",
    "print('Count of records being examined for each label:')\n",
    "print(df.passed.value_counts())\n",
    "\n",
    "# examine how much data we have for each congress_id\n",
    "print('----------------')\n",
    "print('Value counts for each congress:')\n",
    "print(df.congress_id.value_counts())\n",
    "\n",
    "\n",
    "# filter for most recent congress_ids\n",
    "print('------------------')\n",
    "print('Keeping all congress_ids since the 110th Congress (2007) for analysis...')\n",
    "small_df = df[(df['congress_id'] == '115th') | \n",
    "              (df['congress_id'] == '114th') | \n",
    "              (df['congress_id'] == '113th')| \n",
    "              (df['congress_id'] == '112th')| \n",
    "              (df['congress_id'] == '111th')| \n",
    "              (df['congress_id'] == '110th')]\n",
    "print('------------------')\n",
    "print('Number of records we\\'ll be looking at: {}'.format(small_df.shape[0])')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Created corpus, now tokenizing it...\n",
      "------------------\n",
      "Stripping out stop words, punctuation, and numbers...\n",
      "------------------\n",
      "Lemmatizing...\n",
      "------------------\n",
      "Creating a vocabulary...\n",
      "------------------\n",
      "NLP preprocessing complete ...\n",
      "------------------\n",
      "Creating train-test split and vectorizing ...\n"
     ]
    }
   ],
   "source": [
    "############################## NLP\n",
    "############################## NLP\n",
    "############################## NLP\n",
    "# create a corpus\n",
    "documents = list(small_df['body'])\n",
    "\n",
    "# remove numbers\n",
    "documents = list(map(lambda x: ' '.join(re.split('[,_\\d]+', x)), documents))\n",
    "\n",
    "# tokenize the corpus\n",
    "print('------------------')\n",
    "print('Created corpus, now tokenizing it...')\n",
    "corpus = [word_tokenize(content.lower()) for content in documents]\n",
    "\n",
    "# strip out the stop words from each \n",
    "print('------------------')\n",
    "print('Stripping out stop words, punctuation, and numbers...')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['mr', 'ms', 'mrs', 'said', 'year', 'would', 'could', 'also', 'shall', '_______________________________________________________________________'])\n",
    "# print(stop_words)\n",
    "corpus = [[token for token in doc if token not in stop_words] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# strip out the punctuation\n",
    "punc = set(string.punctuation)\n",
    "# print(punc)\n",
    "corpus = [[token for token in doc if token not in punc] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# strip out the punctuation\n",
    "string.digits\n",
    "\n",
    "\n",
    "# lemmatize (and maybe stem?)\n",
    "print('------------------')\n",
    "print('Lemmatizing...')\n",
    "lemmer = WordNetLemmatizer()\n",
    "corpus = [[lemmer.lemmatize(word) for word in doc] for doc in corpus]\n",
    "# corpus[0]\n",
    "\n",
    "# build a vocabulary\n",
    "print('------------------')\n",
    "print('Creating a vocabulary...')\n",
    "vocab_set = set()\n",
    "[[vocab_set.add(token) for token in tokens] for tokens in corpus]\n",
    "vocab = list(vocab_set)\n",
    "# vocab[100000:100020]\n",
    "\n",
    "# # for later model...\n",
    "# # examine n-grams...\n",
    "# # bigrams (two words side-by-side)\n",
    "# print('------------------')\n",
    "# print('Creating lists of bigrams, trigrams, skipgrams, etc...')\n",
    "# bigrams = [list(ngrams(sequence = doc, n = 2)) for doc in corpus]\n",
    "# trigrams = [list(ngrams(sequence = doc, n = 3)) for doc in corpus]\n",
    "# #... more?\n",
    "\n",
    "# # skipgrams (n-grams that skip k words)\n",
    "# skipgrams = [list(skipgrams(sequence = doc, n = 2, k = 1)) for doc in corpus]\n",
    "\n",
    "\n",
    "# rejoin each doc in corpus so each doc is a single string\n",
    "corpus = [' '.join(tokens) for tokens in corpus]\n",
    "\n",
    "print('------------------')\n",
    "print('NLP preprocessing complete ...')\n",
    "\n",
    "print('------------------')\n",
    "print('Creating train-test split and vectorizing ...')\n",
    "X = corpus\n",
    "y = small_df['passed'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a bag of words. CountVectorizer allows us to build a term frequency matrix\n",
    "# print('------------------')\n",
    "# print('Using CountVectorizer to create a term frequency matrix...')\n",
    "# cv = CountVectorizer(stop_words = 'english')\n",
    "# bag_of_words = cv.fit(X_train)\n",
    "\n",
    "# # create a feature dictionary with indices\n",
    "# # The same can be done with tfidfvectorizer\n",
    "# print('------------------')\n",
    "# print('Creating vocabulary and feature list...')\n",
    "# feature_dict = bag_of_words.vocabulary_\n",
    "# # print(feature_dict)\n",
    "\n",
    "# # create an alphabetical feature list\n",
    "# feature_list = bag_of_words.get_feature_names()\n",
    "# # print(feature_list)\n",
    "\n",
    "# # convert to a term frequency matrix. This is a sparse array. \n",
    "# term_freq_matrix = cv.fit_transform(X_train).toarray()\n",
    "# # print(term_freq_matrix)\n",
    "\n",
    "a_list = []\n",
    "r_list = []\n",
    "p_list = []\n",
    "\n",
    "\n",
    "# TF-IDF vectorizer\n",
    "for i in range(200, 4200, 200):\n",
    "    \n",
    "    tfvect = TfidfVectorizer(max_features = i)\n",
    "    X_train_vec = tfvect.fit_transform(X_train)\n",
    "    X_test_vec = tfvect.transform(X_test)\n",
    "\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_vec, y_train)\n",
    "    y_pred = nb.predict(X_test_vec).astype('int')\n",
    "    \n",
    "    a_score = accuracy_score(y_test, y_pred)\n",
    "    r_score = recall_score(y_test, y_pred)\n",
    "    p_score = precision_score(y_test, y_pred)\n",
    "    \n",
    "    a_list.append(a_score)\n",
    "    r_list.append(r_score)\n",
    "    p_list.append(p_score)\n",
    "\n",
    "    \n",
    "#     print('--------------------')\n",
    "#     print('--> Multinomial Naive Bayes predictions after TFIDFVectorizer')\n",
    "#     print(' --> max_features: {}'.format(i))\n",
    "#     print('Accuracy Score: {:.2f}'.format(a_score))\n",
    "#     print('Recall Score: {:.2f}'.format(r_score))\n",
    "#     print('Precision Score: {:.2f}'.format(p_score))\n",
    "#     print('Confusion Matrix:')\n",
    "#     print(confusion_matrix(y_test, y_pred, labels = [1, 0]))\n",
    "#     print('--------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_it(x, y_list, name, labels):\n",
    "    x = x\n",
    "    fig = plt.figure(figsize = (16, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(name, fontdict = {'fontsize': 20})\n",
    "    for y in y_list:\n",
    "        plt.plot(x, y)\n",
    "    plt.legend(labels)\n",
    "\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show graph of scores vs. number of max_features \n",
    "x = np.arange(200, 4200, 200)\n",
    "plot_list = [a_list, r_list, p_list]\n",
    "graph_name = 'Multinomial Naive Bayes Scores vs. Number of Features with Max Frequency in Corpus'\n",
    "labels = ['accuracy score', 'recall score', 'precision score']\n",
    "\n",
    "plot_it(x, plot_list, graph_name, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = []\n",
    "r_list = []\n",
    "p_list = []\n",
    "\n",
    "for i in range(0, 11):\n",
    "    a = .1 * i\n",
    "    tfvect = TfidfVectorizer(max_features = 5000)\n",
    "    X_train_vec = tfvect.fit_transform(X_train)\n",
    "    X_test_vec = tfvect.transform(X_test)\n",
    "\n",
    "    nb = MultinomialNB(alpha = a)\n",
    "    nb.fit(X_train_vec, y_train)\n",
    "    y_pred = nb.predict(X_test_vec).astype('int')\n",
    "    \n",
    "    r_score = recall_score(y_test, y_pred)\n",
    "    \n",
    "    a_list.append(a_score)\n",
    "    r_list.append(r_score)\n",
    "    p_list.append(p_score)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show graph of scores vs. number of max_features \n",
    "x = np.arange(0, 1.1, .1)\n",
    "plot_list = [r_list]\n",
    "graph_name = 'Multinomial Naive Bayes Scores vs. Alpha at 5000 Features'\n",
    "labels = ['accuracy score', 'recall score', 'precision score']\n",
    "\n",
    "plot_it(x, plot_list, graph_name, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after determining the ideal number of features, vectorize again with the specified number\n",
    "# to create the vocabulary for NMF\n",
    "tfvect = TfidfVectorizer(max_features = 2800)\n",
    "X_train_vec = tfvect.fit_transform(X_train)\n",
    "X_test_vec = tfvect.transform(X_test)\n",
    "\n",
    "vocabulary = tfvect.get_feature_names()\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec, y_train)\n",
    "y_pred = nb.predict(X_test_vec).astype('int')\n",
    "y_pred_proba = nb.predict_proba(X_test_vec)\n",
    "\n",
    "a_score = accuracy_score(y_test, y_pred)\n",
    "r_score = recall_score(y_test, y_pred)\n",
    "p_score = precision_score(y_test, y_pred)\n",
    "\n",
    "print('--------------------')\n",
    "print('--> Multinomial Naive Bayes predictions after TFIDFVectorizer')\n",
    "print(' --> max_features: {}'.format(i))\n",
    "print('Accuracy Score: {:.2f}'.format(a_score))\n",
    "print('Recall Score: {:.2f}'.format(r_score))\n",
    "print('Precision Score: {:.2f}'.format(p_score))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred, labels = [1, 0]))\n",
    "print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use non-negative matrix factorization to find the ideal number of topics\n",
    "r_errors = []\n",
    "for i in range(3, 20):\n",
    "    nmf = NMF(n_components = i, alpha = 0.0)\n",
    "    W = nmf.fit_transform(X_train_vec)\n",
    "    H = nmf.components_\n",
    "    r_errors.append(nmf.reconstruction_err_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(3, 20)\n",
    "graph_name = 'NMF Reconstruction Error vs. Number of Latent Features'\n",
    "plot_it(x, [r_errors], graph_name, ['recons error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(H.shape[0]):\n",
    "    top_hits = H[topic].argsort()[:-100:-1].astype(int)\n",
    "    top_words = [vocabulary[hits] for hits in top_hits]\n",
    "    print('-------------')\n",
    "    print('Topic {}'.format(topic + 1))\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(vectorizer, vectors, data, words, n):\n",
    "    '''\n",
    "    Print out the top 10 words by three different sorting mechanisms:\n",
    "        * average tf-idf score\n",
    "        * total tf-idf score\n",
    "        * highest TF score across corpus\n",
    "    '''\n",
    "#     words = vectorizer.get_feature_names()\n",
    "\n",
    "    # Top 10 words by average tfidf\n",
    "    # Take the average of each column, denoted by axis=0\n",
    "    avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "    print (\"top %d by average tf-idf\" % n)\n",
    "    print (get_top_values(avg, n, words))\n",
    "    print ('------------------------')\n",
    "\n",
    "    # Top 10 words by total tfidf\n",
    "    total = np.sum(vectors, axis=0)\n",
    "    print (\"top %d by total tf-idf\" % n)\n",
    "    print (get_top_values(total, n, words))\n",
    "    print ('------------------------')\n",
    "\n",
    "    # Top 10 words by TF\n",
    "    vectorizer2 = TfidfVectorizer(use_idf=False)\n",
    "    # make documents into one giant document for this purpose\n",
    "    vectors2 = vectorizer2.fit_transform([\" \".join(data)]).toarray()\n",
    "    print (\"top %d by tf across all corpus\" % n)\n",
    "    print (get_top_values(vectors2[0], n, words))\n",
    "    print ('------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values. Return\n",
    "    the labels for each of these indices.\n",
    "\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n(tfvect, X_train_vec, corpus, feature_list, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # an option for large set\n",
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# hv = HashingVectorizer(n_features=10)\n",
    "# features = hv.transform(corpus)\n",
    "# print(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
