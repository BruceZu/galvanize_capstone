{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vote_scraper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vote_scraper.py\n",
    "from pymongo import MongoClient\n",
    "import pprint \n",
    "import pandas as pd \n",
    "import copy\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from time import sleep\n",
    "import warnings\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "import boto\n",
    "\n",
    "def write_json_file(obj, path):\n",
    "    '''Dump an object and write it out as json to a file'''\n",
    "    f = codecs.open(path, 'a', 'utf-8')\n",
    "    json_record = json.dumps(obj, ensure_ascii = False)\n",
    "    f.write(json_record + '\\n')\n",
    "    f.close\n",
    "    \n",
    "\n",
    "def get_all_votes(date_range, root_url):\n",
    "    print('_______________')\n",
    "    print('Beginning iterations for House summary data for years {} to {}'.format(min(date_range), max(date_range)))\n",
    "    print('_______________')\n",
    "    for yr in date_range:\n",
    "        site_url = '{}/{}/index.asp'.format(root_url, yr)\n",
    "        req = requests.get(site_url)\n",
    "        tstamp = datetime.datetime.now().strftime('%m-%d-%Y %H:%M:%S')\n",
    "        stat_code = req.status_code\n",
    "        if stat_code != 200:\n",
    "            print('_______________')\n",
    "            print('_______________')\n",
    "            print('Error requesting {}'.format(site_url))\n",
    "            print('Request Status Code: {}, {}'.format(stat_code, tstamp))\n",
    "            sleep(3)\n",
    "            \n",
    "        if stat_code == 200:            \n",
    "            final_roll = get_final_roll_id(root_url, yr)\n",
    "            get_table_summary(root_url, yr, final_roll)\n",
    "\n",
    "    print('_______________')\n",
    "    print('_______________')\n",
    "    print('Iterations through years {} to {} of House summary data complete'.format(min(date_range), max(date_range)))\n",
    "    print('Last url requested: {}'.format(site_url))\n",
    "    print(\"Examine output above for occurrences in request errors, if any.\")\n",
    "\n",
    "    \n",
    "def get_final_roll_id(site_url_root, yr):\n",
    "    site_url = '{}/{}/index.asp'.format(site_url_root, yr)\n",
    "    req = requests.get(site_url)\n",
    "    stat_code = req.status_code\n",
    "\n",
    "    # use BeautifulSoup to find the data we need.\n",
    "    soup = BeautifulSoup(req.content, 'lxml')\n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # initial request of webpage will show the final table with the most recent roll call votes\n",
    "    # get the largest value of roll for iteration\n",
    "    final_roll = int(rows[1].find_all('a')[0].text.strip())\n",
    "    print('Year: {}'.format(yr))\n",
    "    print('\\tFinal Roll ID: {}'.format(final_roll))\n",
    "    \n",
    "    return final_roll\n",
    "\n",
    "\n",
    "def get_table_summary(root_url, yr, final_roll_id):\n",
    "    # get roll summaries from tables from links at index on bottom left\n",
    "    indx_list = []\n",
    "    for i in range(0, final_roll_id + 1):\n",
    "        if i%100 == 0:\n",
    "            indx_list.append('{}'.format(str(i).zfill(3)))\n",
    "    \n",
    "    for indx in indx_list:\n",
    "        vote_table_url = '{}/{}/ROLL_{}.asp'.format(root_url, yr, indx)\n",
    "        req = requests.get(vote_table_url)\n",
    "        stat_code = req.status_code\n",
    "\n",
    "        if stat_code != 200:\n",
    "            print('_______________')\n",
    "            print('_______________')\n",
    "            print(site_url)\n",
    "            print('Request Status Code: {}, {}'.format(stat_code, tstamp))\n",
    "\n",
    "        if stat_code == 200:\n",
    "            # use BeautifulSoup to find the data we need.\n",
    "            soup = BeautifulSoup(req.content, 'lxml')\n",
    "            table = soup.find('table')            \n",
    "            rows = table.find_all('tr')\n",
    "            \n",
    "            outfile = '../data/vote_results_{}.jsonl'.format(yr)\n",
    "            append_rows_to_file(rows, yr, outfile)\n",
    "\n",
    "            \n",
    "    print('\\tIterations through rolls for year {} complete.'.format(yr))\n",
    "    print('\\tLast url: {}'.format(vote_table_url))\n",
    "    print(\"\\tExamine output above for occurrences in request errors, if any.\")\n",
    "    print('_______________')\n",
    "\n",
    "    \n",
    "\n",
    "def append_rows_to_file(rows, yr, filename):\n",
    "    # all_rows = []\n",
    "    empty_row = {\n",
    "                \"year\": None,\n",
    "                \"roll\": None, \n",
    "                \"date\": None, \n",
    "                \"issue\": None,\n",
    "                \"question\": None,\n",
    "                \"result\": None,\n",
    "                \"description\": None, \n",
    "                \"vote_results\": None,\n",
    "                }\n",
    "\n",
    "    # skip the header when reading table\n",
    "    for row in rows[1:]:\n",
    "        new_row = copy.copy(empty_row)\n",
    "        columns = row.find_all('td')\n",
    "        new_row['year'] = yr\n",
    "        new_row['roll'] = columns[0].text.strip()\n",
    "        new_row['date'] = columns[1].text.strip()\n",
    "        new_row['issue'] = columns[2].text.strip()\n",
    "        new_row['question'] = columns[3].text.strip()\n",
    "        new_row['result'] = columns[4].text.strip()\n",
    "        new_row['description'] = columns[5].text.strip()\n",
    "        \n",
    "        results = get_vote_results(yr, int(new_row['roll']))\n",
    "        new_row['vote_results'] = results\n",
    "\n",
    "    #     all_rows.append(new_row)\n",
    "        write_json_file(new_row, filename)\n",
    "\n",
    "\n",
    "# this one should get the actual vote results\n",
    "def get_vote_results(yr, roll_id):\n",
    "    # get vote results for a single roll id\n",
    "    root_url = 'http://clerk.house.gov/evs'\n",
    "    \n",
    "    # convert roll id to 3-digits for url\n",
    "    three_digit_roll = '{}'.format(str(roll_id).zfill(3))\n",
    "\n",
    "    vote_table_url = '{}/{}/roll{}.xml'.format(root_url, yr, three_digit_roll)\n",
    "    req = requests.get(vote_table_url)\n",
    "    stat_code = req.status_code\n",
    "\n",
    "    # print verification that iterator is working\n",
    "    if roll_id%100 == 0:\n",
    "        print('\\t\\t... working ... ... ... ... ... ...')\n",
    "        print('\\t\\t... getting results for Roll ID {}'.format(roll_id))\n",
    "\n",
    "    if stat_code != 200:\n",
    "        print('_______________')\n",
    "        print('_______________')\n",
    "        print('\\t\\tError in retrieving vote results for {}'.format(site_url))\n",
    "        print('\\t\\tRequest Status Code: {}, {}'.format(stat_code, tstamp))\n",
    "\n",
    "    if stat_code == 200:\n",
    "        # use BeautifulSoup to find the data we need.\n",
    "        soup = BeautifulSoup(req.content, 'lxml')\n",
    "        recorded_votes = soup.find_all('recorded-vote')\n",
    "\n",
    "        empty_vote = {\n",
    "                    'name_id': None, \n",
    "                    'name': None,\n",
    "                    'party': None,\n",
    "                    'state': None,\n",
    "                    'vote': None\n",
    "                    }\n",
    "\n",
    "        vote_info = []\n",
    "\n",
    "        for line in recorded_votes:\n",
    "            new_vote = copy.copy(empty_vote)\n",
    "            legislator = line.find('legislator')\n",
    "            new_vote['name'] = legislator.text\n",
    "            new_vote['vote'] = line.find('vote').text\n",
    "\n",
    "            for k in list(empty_vote.keys()):\n",
    "                if k in list(legislator.attrs.keys()):\n",
    "                    new_vote[k] = legislator[k]\n",
    "\n",
    "            vote_info.append(new_vote)\n",
    "\n",
    "        return vote_info\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    house_url_root = 'http://clerk.house.gov/evs'\n",
    "\n",
    "    # date_range = list(range(1990, 2019))\n",
    "    date_range = list(range(1990, 1992))\n",
    "\n",
    "    get_all_votes(date_range, house_url_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AWS S3 connection\n",
    "conn = boto.connect_s3()\n",
    "# print(conn)\n",
    "\n",
    "conn.get_all_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bucket for all of our project data\n",
    "# not needed after creation\n",
    "# legislation_bucket = conn.create_bucket('galvcap-legislation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legislation_bucket.get_all_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to S3\n",
    "roll_summary_file = legislation_bucket.new_key('roll_summaries.txt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
