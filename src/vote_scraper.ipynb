{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile vote_scraper.py\n",
    "from pymongo import MongoClient\n",
    "import pprint \n",
    "import pandas as pd \n",
    "import copy\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from time import sleep\n",
    "import warnings\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client.bills\n",
    "vote_records = db.vote_records\n",
    "\n",
    "# the 101st Congress (1989 - 1990) starts on pg 1011 for pageSize=250\n",
    "house_url_root = 'http://clerk.house.gov/evs'\n",
    "\n",
    "date_range = list(range(1990, 2019))\n",
    "\n",
    "\n",
    "def house_year_iterator(date_range, root_url):\n",
    "    print('_______________')\n",
    "    print('Beginning iterations for House data for years {} to {}'.format(min(date_range), max(date_range)))\n",
    "    print('_______________')\n",
    "    for yr in date_range:\n",
    "        site_url = '{}/{}/index.asp'.format(root_url, yr)\n",
    "        req = requests.get(site_url)\n",
    "        tstamp = datetime.datetime.now().strftime('%m-%d-%Y %H:%M:%S')\n",
    "        stat_code = req.status_code\n",
    "        if stat_code != 200:\n",
    "            print('_______________')\n",
    "            print('_______________')\n",
    "            print('Error requesting {}'.format(site_url))\n",
    "            print('Request Status Code: {}, {}'.format(stat_code, tstamp))\n",
    "            sleep(3)\n",
    "\n",
    "    print('Iterations through years of House data complete')\n",
    "    print('Last url requested: {}'.format(site_url))\n",
    "    print(\"Examine output above for occurrences in request errors, if any.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "Beginning iterations for House data for years 1990 to 2018\n",
      "Iterations through years of House data complete\n",
      "Last url requested: http://clerk.house.gov/evs/2018/index.asp\n",
      "Examine output above for occurrences in request errors, if any.\n"
     ]
    }
   ],
   "source": [
    "house_year_iterator(date_range, house_url_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________\n",
      "_______________\n",
      "http://clerk.house.gov/evs/1990/index.asp\n",
      "Request Status Code: 200, 12-10-2018 11:09:28\n"
     ]
    }
   ],
   "source": [
    "# test code with year 1990 for House votes\n",
    "site_url_root = 'http://clerk.house.gov/evs'\n",
    "yr = '1990'\n",
    "\n",
    "site_url = '{}/{}/index.asp'.format(site_url_root, yr)\n",
    "req = requests.get(site_url)\n",
    "tstamp = datetime.datetime.now().strftime('%m-%d-%Y %H:%M:%S')\n",
    "stat_code = req.status_code\n",
    "print('_______________')\n",
    "print('_______________')\n",
    "print(site_url)\n",
    "print('Request Status Code: {}, {}'.format(stat_code, tstamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BeautifulSoup to find the data we need.\n",
    "soup = BeautifulSoup(req.content, 'lxml')\n",
    "# print(soup.prettify())\n",
    "\n",
    "table = soup.find('table')\n",
    "# print(table.prettify())\n",
    "\n",
    "rows = table.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial request of webpage will show the final table with the most recent roll call votes\n",
    "# get the largest value of roll for iteration\n",
    "final_roll = int(rows[1].find_all('a')[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_url_head = 'http://clerk.house.gov/evs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get voting data by iterating through roll ids\n",
    "for i in range(1, final_roll + 1):\n",
    "    # convert roll id to 3-digits for url\n",
    "    three_digit_roll = '{}'.format(str(i).zfill(3))\n",
    "    vote_table_url = '{}/{}/roll{}.xml'.format(vote_url_head, yr, three_digit_roll)\n",
    "#     print(vote_table_url)\n",
    "    req = requests.get(vote_table_url)\n",
    "    stat_code = req.status_code\n",
    "    \n",
    "    # print verification that iterator is working\n",
    "    if i%100 == 0:\n",
    "        print('...................')\n",
    "        print('On Roll ID {}'.format(i))\n",
    "        \n",
    "    if stat_code != 200:\n",
    "        print('_______________')\n",
    "        print('_______________')\n",
    "        print(site_url)\n",
    "        print('Request Status Code: {}, {}'.format(stat_code, tstamp))\n",
    "\n",
    "#     sleep(3)\n",
    "        \n",
    "print('Iterations through rolls for year {} complete.'.format(yr))\n",
    "print('Last url: {}'.format(vote_table_url))\n",
    "print(\"Examine output above for occurrences in request errors, if any.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "empty_row = {\n",
    "            \"year\": yr,\n",
    "            \"roll\": None, \n",
    "            \"date\": None, \n",
    "            \"issue\": None,\n",
    "            \"question\": None,\n",
    "            \"result\": None,\n",
    "            \"description\": None\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip the header when reading table\n",
    "for row in rows[1:]:\n",
    "    new_row = copy.copy(empty_row)\n",
    "    columns = row.find_all('td')\n",
    "    new_row['roll'] = columns[0].text.strip()\n",
    "    new_row['issue'] = columns[2].text.strip()\n",
    "    all_rows.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if req.status_code == 200:\n",
    "#################\n",
    "browser.get(site_url)\n",
    "soup = BeautifulSoup(browser.page_source, 'lxml')\n",
    "# print(soup.prettify())\n",
    "div = soup.find('div', {'class':'row'})\n",
    "print(div.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add page page html to mongo\n",
    "collection_name.insert_one({'lxml': req.content})\n",
    "\n",
    "\n",
    "# print result of load\n",
    "with open('data/load_results.txt', 'a') as f:\n",
    "    f.writelines('{}, {}, {}\\n'.format(site_url, stat_code, tstamp))\n",
    "f.close()\n",
    "\n",
    "\n",
    "##################\n",
    "# else: \n",
    "#     print('failed to get {}'.format(site_url))\n",
    "#     # print result of load\n",
    "#     with open('data/logs/load_results.txt', 'a') as f:\n",
    "#         f.writelines('{}, {}, {}\\n'.format(site_url, stat_code, tstamp))\n",
    "#     f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
